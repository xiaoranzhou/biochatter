{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Generative AI models have shown tremendous usefulness in increasing accessibility and automation of a wide range of tasks. Yet, their application to the biomedical domain is still limited, in part due to the lack of a common framework for deploying, testing, and evaluating the diverse models and auxiliary technologies that are needed. <code>biochatter</code> is a Python package implementing a generic backend library for the connection of biomedical applications to conversational AI.  Described in this preprint and used in ChatGSE, which is being developed here.</p> <p>BioChatter is part of the BioCypher ecosystem,  connecting natively to BioCypher knowledge graphs. The BioChatter paper is being written here.</p>"},{"location":"#installation","title":"Installation","text":"<p>To use the package, install it from PyPI, for instance using pip (<code>pip install biochatter</code>) or Poetry (<code>poetry add biochatter</code>).</p>"},{"location":"#extras","title":"Extras","text":"<p>The package has some optional dependencies that can be installed using the following extras (e.g. <code>pip install biochatter[xinference]</code>):</p> <ul> <li> <p><code>xinference</code>: support for querying open-source LLMs through Xorbits Inference</p> </li> <li> <p><code>podcast</code>: support for podcast text-to-speech (for the free Google TTS; the paid OpenAI TTS can be used without this extra)</p> </li> <li> <p><code>streamlit</code>: support for streamlit UI functions (used in ChatGSE)</p> </li> </ul> <p> </p>"},{"location":"benchmark/","title":"Benchmarking","text":"<p>For trustworthy application of LLMs to real-world and biomedical problems, it is imperative to understand their performance and limitations.  We need to constantly evaluate the multitude of combinations of individual models and versions, their parameters (e.g., temperature), prompt sets, databases and vector databases, and diverse application scenarios. To this end, we are maintaining a living benchmarking framework that allows us to continuously compare the performance of different models and configurations on a variety of tasks.</p> <p>The benchmark uses the pytest framework to orchestrate the evaluation of a number of models on a number of tasks. The benchmark is run on a regular basis, and the results are published on the BioChatter website. (Currently in development.) The benchmarking suite can be found in the <code>benchmark</code> directory of the BioChatter repository. It can be executed using standard pytest syntax, e.g., <code>poetry run pytest benchmark</code>.</p> <p>To allow flexible extension of the benchmark, we have implemeted a modular test framework that uses pytest fixtures to allow easy addition of new models and tasks. All setup is done in the <code>conftest.py</code> file in the <code>benchmark</code> directory. For management of result files, we use the <code>RESULT_FILES</code> list in the <code>conftest.py</code> file. Each benchmarking module imports the <code>RESULT_FILES</code> list and retrieves the path to the result file it should write to. The result files are simple CSVs that can be found in <code>benchmark/results</code> and contain scores for all executed combination of parameters.</p> <p>To achieve modularity, we use pytest fixtures and parametrization. For instance, to add a new model, we can modify the <code>MODEL_NAMES</code> list in the query generation test module, or the <code>EMBEDDING_MODELS</code> and <code>CHUNK_SIZES</code> lists in the vector database test module. The environment that runs the benchmark needs to make available all prerequisites for the different modules. For instance, the tasks requiring connection to an LLM need to provide the necessary credentials and API keys, or a connection to a self-hosted model. Likewise, the benchmarks of retrieval augmented generation (RAG) processes require a connection to the RAG agent, e.g., a vector database.</p>"},{"location":"benchmark/#calibration","title":"Calibration","text":"<p>To ensure valid assessment of LLM performance, we need to ensure calibration and technical validity of the benchmarking framework. More recent LLMs in particular may be problematic when using publicly available benchmark datasets, since they could have been used for training the model. This is particularly relevant in closed-source (e.g., OpenAI) models. Thus, we need to consider strategies for avoiding contamination, such as hand-crafting datasets, carefully testing for contamination, or using perturbation strategies to generate new datasets from existing ones. Advanced scenarios could utilise LLMs as \"examiners,\" allowing more flexible test design and free-form answers. There is much research into these phenomena, all of which should be considered in the maintenance of this testing framework.</p>"},{"location":"benchmark/#aspects-of-benchmarking","title":"Aspects of benchmarking","text":"<p>In the following, we will detail the different aspects of benchmarking that we are currently testing. This is a living document that will be updated as we add new tests and test modules.</p>"},{"location":"benchmark/#models","title":"Models","text":"<p>Naturally the biggest impact on BioChatter performance comes with the model used. However, model versions can have a significant impact, which can be obfuscated by the fact that model names are often not unique. For instance, OpenAI's GPT models often have versions with significantly diverging capabilities and performance. [etc]</p>"},{"location":"benchmark/#prompts","title":"Prompts","text":"<p>As has been recently studied extensively, prompt engineering can make or break the performance of a model on a given task. As such, it is important to test the default prompts we commonly use, as well as a range of variations to determine factors of prompt performance and robustness. As an added complexity, LLMs are often used to generate prompts, which theoretically allows for procedural generation of an infinite number of prompts, as long as time and resources allow.</p>"},{"location":"benchmark/#model-parameters","title":"Model parameters","text":"<p>The parameters of the model can have a significant impact on the performance of the model. We often set model temperature to 0 to provide consistent results, but some applications may benefit from a higher temperature. In testing, we mostly rely on a temperature of 0 due to the complexity of testing highly variable results in most cases.</p>"},{"location":"benchmark/#databases","title":"Databases","text":"<p>An important facet of BioChatter and BioCypher is their combination in querying databases. This helps to ameliorate the limitations of LLMs by providing structured and validated knowledge to counteract hallucinations. To ensure the seamless interaction of BioChatter and BioCypher, we need to test the performance of BioChatter on a variety of databases.</p>"},{"location":"benchmark/#vector-databases","title":"Vector databases","text":"<p>Similarly to regular databases, vector databases are an important tool to provide validated knowledge to LLMs. Vector databases bring their own set of parameters and application scenarios, which likewise need to be tested. For instance, the length and overlap of fragments, the embedding algorithms, as well as the semantic search algorithms applied can have an impact on LLM conversation performance.</p>"},{"location":"benchmark/#tasks","title":"Tasks","text":"<p>There is a wide range of tasks that are potentially useful to BioChatter users. To cover most scenarios of research and development use, as well as clinical applications, we test a variety of tasks and LLM personas.</p>"},{"location":"chat/","title":"Basic Usage: Chat","text":"<p>BioChatter provides access to chat functionality via the <code>Conversation</code> class, which is implemented in several child classes (in the <code>llm_connect.py</code> module) to account for differences in APIs of the LLMs.</p>"},{"location":"chat/#setting-up-the-conversation","title":"Setting up the conversation","text":"<p>To start a conversation, we can initialise the Conversation class (here exemplified by GPT):</p> <pre><code>from biochatter.llm_connect import GptConversation\n\nconversation = GptConversation(\n    model_name=\"gpt-3.5-turbo\",\n    prompts={},\n)\n</code></pre> <p>It is possible to supply a dictionary of prompts to the conversation from the outset, which is formatted in a way to correspond to the different roles of the conversation, i.e., primary and correcting models. Prompts with the <code>primary_model_prompts</code> key will be appended to the System Messages of the primary model, and <code>correcting_agent_prompts</code> will be appended to the System Messages of the correction model at setup. If we pass a dictionary without these keys (or an empty one), there will be no system messages appended to the models. They can however be introduced later by using the following method:</p> <pre><code>conversation.append_system_message(\"System Message\")\n</code></pre> <p>Similarly, the user queries (<code>HumanMessage</code>) are passed to the conversation using <code>convo.append_user_message(\"User Message\")</code>. For purposes of keeping track of the conversation history, we can also append the model's responses as <code>AIMessage</code> using <code>convo.append_ai_message</code>. </p>"},{"location":"chat/#querying-the-model","title":"Querying the model","text":"<p>After setting up the conversation in this way, for instance by establishing a flattery component (e.g. 'You are an assistant to a researcher ...'), the model can be queried using the <code>query</code> function.</p> <pre><code>msg, token_usage, correction = conversation.query('Question here')\n</code></pre> <p>Note that a query will automatically append a user message to the message history, so there is no need to call <code>append_user_message()</code> again. The query function returns the actual answer of the model (<code>msg</code>), the token usage statistics reported by the API (<code>token_usage</code>), and an optional <code>correction</code> that contains the opinion of the corrective agent.</p>"},{"location":"kgs/","title":"Connecting Knowledge Graphs","text":"<p>To increase accessibility of databases, we can leverage the BioCypher integration of BioChatter.  In BioCypher, we use a YAML configuration (<code>schema_config.yaml</code>) to specify the contents of the knowledge graph and their ontological associations.  We also generate a more extensive, but essentially similar YAML file during the BioCypher creation of a knowledge graph (<code>schema_info.yaml</code>), which contains more information pertinent to LLM interaction with the database.  The current prototypical implementation of query generation through an LLM is implemented in the <code>prompts.py</code> module on the example of a Neo4j knowledge graph connection.</p>"},{"location":"kgs/#connecting","title":"Connecting","text":"<p>Currently, BioChatter does not handle database connectivity, but simply returns a query for a given language.  The application using BioChatter should establish connectivity and send the query to the database, as is implemented in ChatGSE, for instance.  For a demonstration using a simple Docker compose setup, see the Pole Crime Dataset demo repository.</p>"},{"location":"kgs/#querying","title":"Querying","text":"<p>The generation of a query based on BioCypher configuration files is a multi-step process.  This is partly to account for the limited token input space of some models, and partly to better be able to test and compare the individual steps. We will implement a wrapper function that goes through the steps automatically soon, but for now the steps need to be run individually.</p>"},{"location":"kgs/#setup","title":"Setup","text":"<p>We use the <code>BioCypherPromptEngine</code> class to handle the LLM conversation.</p> <pre><code>from biochatter.prompts import BioCypherPromptEngine\nprompt_engine = BioCypherPromptEngine(\n    schema_config_or_info_path=\"test/schema_info.yaml\"\n)\n</code></pre> <p>This will load the <code>schema_config.yaml</code> or <code>schema_info.yaml</code> (preferred) file and set up the conversation.</p>"},{"location":"kgs/#query-generation","title":"Query generation","text":"<p>Using the <code>generate_query</code> wrapper, we can generate a query from a question and a database language.</p> <pre><code>query = prompt_engine.generate_query(\n    question=\"Which genes are associated with mucoviscidosis?\",\n    database_language=\"Cypher\",\n)\n</code></pre> <p>This will return a query that can be used in the database query language (e.g., Cypher).  This end to end process executes the steps detailed below, namely, entity selection, relationship selection, and property selection, as well as the generation of the final query using the selected components. You can run each of these steps individually, if you want.</p>"},{"location":"kgs/#entity-selection","title":"Entity selection","text":"<p>Starting from the <code>schema_config.yaml</code> or <code>schema_info.yaml</code> (preferred) file, we first have the model decide which entities in the database are relevant to the user's question.</p> <pre><code>success = prompt_engine._select_entities(\n    question=\"Which genes are associated with mucoviscidosis?\"\n)\n</code></pre> <p>This will select a number of entities from the database schema to be used subsequently, and return True or False to indicate success.</p>"},{"location":"kgs/#relationship-selection","title":"Relationship selection","text":"<p>Next, we will use the entities determined in the first step to select relationships between them.  The entities selected in the first step will be stored in the <code>selected_entities</code> attribute of the <code>BioCypherPromptEngine</code> instance, and the question is stored in the <code>question</code> attribute.  Both are automatically used to select relationships.</p> <pre><code>success = prompt_engine._select_relationships()\n</code></pre>"},{"location":"kgs/#property-selection","title":"Property selection","text":"<p>To not unnecessarily waste token input space, we are only interested in selecting properties of entities that are of interest given the question asked. We do so in the third step, which uses the entities and relationships determined in the first steps.  Again, <code>question</code>, <code>selected_entities</code>, and <code>selected_relationships</code> are automatically used to select properties.</p> <pre><code>success = prompt_engine._select_properties()\n</code></pre> <p>This will select a number of properties to be used in the query, and also return True or False to indicate success.</p>"},{"location":"kgs/#query-generation_1","title":"Query generation","text":"<p>Finally, we can use the entities and relationships, as well as the selected properties, to ask the LLM to generate a query in the desired language.</p> <pre><code>query = prompt_engine._generate_query(\n    question=\"Which genes are associated with mucoviscidosis?\",\n    entities=[\"Gene\", \"Disease\"],\n    relationships=[\"GeneToDiseaseAssociation\"],\n    properties={\"Disease\": [\"name\", \"ICD10\", \"DSM5\"]},\n    database_language=\"Cypher\",\n)\n</code></pre> <p>This will (hopefully) return a query that can be used in the database query language (e.g., Cypher).</p>"},{"location":"kgs/#query-interaction","title":"Query interaction","text":"<p>As an optional follow-up, you can interact with the returned query using the  <code>BioCypherQueryHandler</code> class (<code>query_interaction.py</code>). It takes the query,  the original question and the KG information so that the interaction is still aware of the KG. </p> <pre><code>from biochatter.query_interaction import BioCypherQueryHandler\nquery_handler = BioCypherQueryHandler(\n    query=query,\n    query_lang=\"Cypher\",\n    kg_selected={\n        entities: [\"Gene\", \"Disease\"],\n        relationships: [\"GeneToDiseaseAssociation\"],\n        properties: {\"Disease\": [\"name\", \"ICD10\", \"DSM5\"]}\n    },\n    question=\"Which genes are associated with mucoviscidosis?\"\n)\n</code></pre>"},{"location":"kgs/#explanation","title":"Explanation","text":"<p>You can retrieve an explanation of the returned query with:</p> <pre><code>explanation = query_handler.explain_query()\n</code></pre>"},{"location":"kgs/#updating","title":"Updating","text":"<p>Alternatively, you can ask the LLM for an update of the query with:</p> <pre><code>request = \"Only return 10 results and sort them alphabetically\"\nexplanation = query_handler.update_query(request)\n</code></pre> <p>NB: for updates, it might sometimes be relevant that all the KG  enitites/relationships/properties are known to the LLM instead  of only those that were selected to be relevant for the original question. For this, you can optionally pass them as input to the query handler  with <code>kg</code> (similar to <code>kg_selected</code>).</p> <p>(Tip: the prompt_engine object contains both the selected and non-selected as attributes)</p>"},{"location":"llm_connect-reference/","title":"LLM connectivity module","text":"<p>Here we handle connections to various LLM services, proprietary and open source.</p>"},{"location":"llm_connect-reference/#biochatter.llm_connect.AzureGptConversation","title":"<code>AzureGptConversation</code>","text":"<p>             Bases: <code>GptConversation</code></p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class AzureGptConversation(GptConversation):\n    def __init__(\n        self,\n        deployment_name: str,\n        model_name: str,\n        prompts: dict,\n        correct: bool = True,\n        split_correction: bool = False,\n        rag_agent: DocumentEmbedder = None,\n        version: Optional[str] = None,\n        base_url: Optional[str] = None,\n    ):\n        \"\"\"\n        Connect to Azure's GPT API and set up a conversation with the user.\n        Extends GptConversation.\n\n        Args:\n            deployment_name (str): The name of the Azure deployment to use.\n\n            model_name (str): The name of the model to use. This is distinct\n                from the deployment name.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each\n                sentence individually.\n\n            rag_agent (DocumentEmbedder): A vector database connection to use for\n                retrieval augmented generation (RAG).\n\n            version (str): The version of the Azure API to use.\n\n            base_url (str): The base URL of the Azure API to use.\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n            rag_agent=rag_agent,\n        )\n\n        self.version = version\n        self.base_url = base_url\n        self.deployment_name = deployment_name\n\n    def set_api_key(self, api_key: str, user: Optional[str] = None):\n        \"\"\"\n        Set the API key for the Azure API. If the key is valid, initialise the\n        conversational agent. No user stats on Azure.\n\n        Args:\n            api_key (str): The API key for the Azure API.\n\n        Returns:\n            bool: True if the API key is valid, False otherwise.\n        \"\"\"\n\n        try:\n            self.chat = AzureChatOpenAI(\n                deployment_name=self.deployment_name,\n                model_name=self.model_name,\n                openai_api_version=self.version,\n                openai_api_base=self.base_url,\n                openai_api_key=api_key,\n                temperature=0,\n            )\n            # TODO this is the same model as the primary one; refactor to be\n            # able to use any model for correction\n            self.ca_chat = AzureChatOpenAI(\n                deployment_name=self.deployment_name,\n                model_name=self.model_name,\n                openai_api_version=self.version,\n                openai_api_base=self.base_url,\n                openai_api_key=api_key,\n                temperature=0,\n            )\n\n            test = self.chat.generate([[HumanMessage(content=\"Hello\")]])\n\n            return True\n\n        except openai._exceptions.AuthenticationError as e:\n            return False\n\n    def _update_usage_stats(self, model: str, token_usage: dict):\n        \"\"\"\n        We do not track usage stats for Azure.\n        \"\"\"\n        return\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.AzureGptConversation.__init__","title":"<code>__init__(deployment_name, model_name, prompts, correct=True, split_correction=False, rag_agent=None, version=None, base_url=None)</code>","text":"<p>Connect to Azure's GPT API and set up a conversation with the user. Extends GptConversation.</p> <p>Parameters:</p> Name Type Description Default <code>deployment_name</code> <code>str</code> <p>The name of the Azure deployment to use.</p> required <code>model_name</code> <code>str</code> <p>The name of the model to use. This is distinct from the deployment name.</p> required <code>prompts</code> <code>dict</code> <p>A dictionary of prompts to use for the conversation.</p> required <code>split_correction</code> <code>bool</code> <p>Whether to correct the model output by splitting the output into sentences and correcting each sentence individually.</p> <code>False</code> <code>rag_agent</code> <code>DocumentEmbedder</code> <p>A vector database connection to use for retrieval augmented generation (RAG).</p> <code>None</code> <code>version</code> <code>str</code> <p>The version of the Azure API to use.</p> <code>None</code> <code>base_url</code> <code>str</code> <p>The base URL of the Azure API to use.</p> <code>None</code> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    deployment_name: str,\n    model_name: str,\n    prompts: dict,\n    correct: bool = True,\n    split_correction: bool = False,\n    rag_agent: DocumentEmbedder = None,\n    version: Optional[str] = None,\n    base_url: Optional[str] = None,\n):\n    \"\"\"\n    Connect to Azure's GPT API and set up a conversation with the user.\n    Extends GptConversation.\n\n    Args:\n        deployment_name (str): The name of the Azure deployment to use.\n\n        model_name (str): The name of the model to use. This is distinct\n            from the deployment name.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each\n            sentence individually.\n\n        rag_agent (DocumentEmbedder): A vector database connection to use for\n            retrieval augmented generation (RAG).\n\n        version (str): The version of the Azure API to use.\n\n        base_url (str): The base URL of the Azure API to use.\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n        rag_agent=rag_agent,\n    )\n\n    self.version = version\n    self.base_url = base_url\n    self.deployment_name = deployment_name\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.AzureGptConversation.set_api_key","title":"<code>set_api_key(api_key, user=None)</code>","text":"<p>Set the API key for the Azure API. If the key is valid, initialise the conversational agent. No user stats on Azure.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for the Azure API.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the API key is valid, False otherwise.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def set_api_key(self, api_key: str, user: Optional[str] = None):\n    \"\"\"\n    Set the API key for the Azure API. If the key is valid, initialise the\n    conversational agent. No user stats on Azure.\n\n    Args:\n        api_key (str): The API key for the Azure API.\n\n    Returns:\n        bool: True if the API key is valid, False otherwise.\n    \"\"\"\n\n    try:\n        self.chat = AzureChatOpenAI(\n            deployment_name=self.deployment_name,\n            model_name=self.model_name,\n            openai_api_version=self.version,\n            openai_api_base=self.base_url,\n            openai_api_key=api_key,\n            temperature=0,\n        )\n        # TODO this is the same model as the primary one; refactor to be\n        # able to use any model for correction\n        self.ca_chat = AzureChatOpenAI(\n            deployment_name=self.deployment_name,\n            model_name=self.model_name,\n            openai_api_version=self.version,\n            openai_api_base=self.base_url,\n            openai_api_key=api_key,\n            temperature=0,\n        )\n\n        test = self.chat.generate([[HumanMessage(content=\"Hello\")]])\n\n        return True\n\n    except openai._exceptions.AuthenticationError as e:\n        return False\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.BloomConversation","title":"<code>BloomConversation</code>","text":"<p>             Bases: <code>Conversation</code></p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class BloomConversation(Conversation):\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        split_correction: bool,\n        rag_agent: DocumentEmbedder = None,\n    ):\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            split_correction=split_correction,\n            rag_agent=rag_agent,\n        )\n\n        self.messages = []\n\n    def set_api_key(self, api_key: str, user: Optional[str] = None):\n        self.chat = HuggingFaceHub(\n            repo_id=self.model_name,\n            model_kwargs={\"temperature\": 1.0},  # \"regular sampling\"\n            # as per https://huggingface.co/docs/api-inference/detailed_parameters\n            huggingfacehub_api_token=api_key,\n        )\n\n        try:\n            self.chat.generate([\"Hello, I am a biomedical researcher.\"])\n            return True\n        except ValueError as e:\n            return False\n\n    def _cast_messages(self, messages):\n        \"\"\"\n        Render the different roles of the chat-based conversation as plain text.\n        \"\"\"\n        cast = \"\"\n        for m in messages:\n            if isinstance(m, SystemMessage):\n                cast += f\"System: {m.content}\\n\"\n            elif isinstance(m, HumanMessage):\n                cast += f\"Human: {m.content}\\n\"\n            elif isinstance(m, AIMessage):\n                cast += f\"AI: {m.content}\\n\"\n            else:\n                raise ValueError(f\"Unknown message type: {type(m)}\")\n\n        return cast\n\n    def _primary_query(self):\n        response = self.chat.generate([self._cast_messages(self.messages)])\n\n        msg = response.generations[0][0].text\n        token_usage = {\n            \"prompt_tokens\": 0,\n            \"completion_tokens\": 0,\n            \"total_tokens\": 0,\n        }\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _correct_response(self, msg: str):\n        return \"ok\"\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation","title":"<code>Conversation</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Use this class to set up a connection to an LLM API. Can be used to set the user name and API key, append specific messages for system, user, and AI roles (if available), set up the general context as well as manual and tool-based data inputs, and finally to query the API with prompts made by the user.</p> <p>The conversation class is expected to have a <code>messages</code> attribute to store the conversation, and a <code>history</code> attribute, which is a list of messages in a specific format for logging / printing.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class Conversation(ABC):\n    \"\"\"\n\n    Use this class to set up a connection to an LLM API. Can be used to set the\n    user name and API key, append specific messages for system, user, and AI\n    roles (if available), set up the general context as well as manual and\n    tool-based data inputs, and finally to query the API with prompts made by\n    the user.\n\n    The conversation class is expected to have a `messages` attribute to store\n    the conversation, and a `history` attribute, which is a list of messages in\n    a specific format for logging / printing.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = True,\n        split_correction: bool = False,\n        rag_agent: DocumentEmbedder = None,\n    ):\n        super().__init__()\n        self.model_name = model_name\n        self.prompts = prompts\n        self.correct = correct\n        self.split_correction = split_correction\n        self.rag_agent = rag_agent\n        self.history = []\n        self.messages = []\n        self.ca_messages = []\n        self.current_statements = []\n\n    def set_user_name(self, user_name: str):\n        self.user_name = user_name\n\n    @abstractmethod\n    def set_api_key(self, api_key: str, user: Optional[str] = None):\n        pass\n\n    def get_prompts(self):\n        return self.prompts\n\n    def set_prompts(self, prompts: dict):\n        self.prompts = prompts\n\n    def set_rag_agent(self, rag_agent: DocumentEmbedder):\n        self.rag_agent = rag_agent\n\n    def append_ai_message(self, message: str):\n        self.messages.append(\n            AIMessage(\n                content=message,\n            ),\n        )\n\n    def append_system_message(self, message: str):\n        self.messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n\n    def append_ca_message(self, message: str):\n        self.ca_messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n\n    def append_user_message(self, message: str):\n        self.messages.append(\n            HumanMessage(\n                content=message,\n            ),\n        )\n\n    def setup(self, context: str):\n        \"\"\"\n        Set up the conversation with general prompts and a context.\n        \"\"\"\n        for msg in self.prompts[\"primary_model_prompts\"]:\n            if msg:\n                self.append_system_message(msg)\n\n        for msg in self.prompts[\"correcting_agent_prompts\"]:\n            if msg:\n                self.append_ca_message(msg)\n\n        self.context = context\n        msg = f\"The topic of the research is {context}.\"\n        self.append_system_message(msg)\n\n    def setup_data_input_manual(self, data_input: str):\n        self.data_input = data_input\n        msg = f\"The user has given information on the data input: {data_input}.\"\n        self.append_system_message(msg)\n\n    def setup_data_input_tool(self, df, input_file_name: str):\n        self.data_input_tool = df\n\n        for tool_name in self.prompts[\"tool_prompts\"]:\n            if tool_name in input_file_name:\n                msg = self.prompts[\"tool_prompts\"][tool_name].format(df=df)\n                self.append_system_message(msg)\n\n    def query(self, text: str, collection_name: Optional[str] = None):\n        self.append_user_message(text)\n\n        if self.rag_agent:\n            if self.rag_agent.use_prompt:\n                self._inject_context(text, collection_name)\n\n        msg, token_usage = self._primary_query()\n\n        if not token_usage:\n            # indicates error\n            return (msg, token_usage, None)\n\n        if not self.correct:\n            return (msg, token_usage, None)\n\n        cor_msg = (\n            \"Correcting (using single sentences) ...\"\n            if self.split_correction\n            else \"Correcting ...\"\n        )\n\n        if st:\n            with st.spinner(cor_msg):\n                corrections = self._correct_query(text)\n        else:\n            corrections = self._correct_query(text)\n\n        if not corrections:\n            return (msg, token_usage, None)\n\n        correction = \"\\n\".join(corrections)\n        return (msg, token_usage, correction)\n\n    def _correct_query(self, msg: str):\n        corrections = []\n        if self.split_correction:\n            nltk.download(\"punkt\")\n            tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n            sentences = tokenizer.tokenize(msg)\n            for sentence in sentences:\n                correction = self._correct_response(sentence)\n\n                if not str(correction).lower() in [\"ok\", \"ok.\"]:\n                    corrections.append(correction)\n        else:\n            correction = self._correct_response(msg)\n\n            if not str(correction).lower() in [\"ok\", \"ok.\"]:\n                corrections.append(correction)\n\n        return corrections\n\n    @abstractmethod\n    def _primary_query(self, text: str):\n        pass\n\n    @abstractmethod\n    def _correct_response(self, msg: str):\n        pass\n\n    def _inject_context(self, text: str, collection_name: Optional[str] = None):\n        \"\"\"\n        Inject the context into the prompt from vector database similarity\n        search. Finds the most similar n text fragments and adds them to the\n        message history object for usage in the next prompt. Uses the document\n        summarisation prompt set to inject the context. The ultimate prompt\n        should include the placeholder for the statements, `{statements}` (used\n        for formatting the string).\n\n        Args:\n            text (str): The user query to be used for similarity search.\n        \"\"\"\n        if not self.rag_agent.used:\n            st.info(\n                \"No document has been analysed yet. To use retrieval augmented \"\n                \"generation, please analyse at least one document first.\"\n            )\n            return\n\n        sim_msg = (\n            f\"Performing similarity search to inject {self.rag_agent.n_results}\"\n            \" fragments ...\"\n        )\n\n        if st:\n            with st.spinner(sim_msg):\n                statements = [\n                    doc.page_content\n                    for doc in self.rag_agent.similarity_search(\n                        text,\n                        self.rag_agent.n_results,\n                    )\n                ]\n        else:\n            statements = [\n                doc.page_content\n                for doc in self.rag_agent.similarity_search(\n                    text,\n                    self.rag_agent.n_results,\n                )\n            ]\n\n        prompts = self.prompts[\"rag_agent_prompts\"]\n        if statements:\n            self.current_statements = statements\n            for i, prompt in enumerate(prompts):\n                # if last prompt, format the statements into the prompt\n                if i == len(prompts) - 1:\n                    self.append_system_message(\n                        prompt.format(statements=statements)\n                    )\n                else:\n                    self.append_system_message(prompt)\n\n    def get_msg_json(self):\n        \"\"\"\n        Return a JSON representation (of a list of dicts) of the messages in\n        the conversation. The keys of the dicts are the roles, the values are\n        the messages.\n\n        Returns:\n            str: A JSON representation of the messages in the conversation.\n        \"\"\"\n        d = []\n        for msg in self.messages:\n            if isinstance(msg, SystemMessage):\n                role = \"system\"\n            elif isinstance(msg, HumanMessage):\n                role = \"user\"\n            elif isinstance(msg, AIMessage):\n                role = \"ai\"\n            else:\n                raise ValueError(f\"Unknown message type: {type(msg)}\")\n\n            d.append({role: msg.content})\n\n        return json.dumps(d)\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.get_msg_json","title":"<code>get_msg_json()</code>","text":"<p>Return a JSON representation (of a list of dicts) of the messages in the conversation. The keys of the dicts are the roles, the values are the messages.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A JSON representation of the messages in the conversation.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def get_msg_json(self):\n    \"\"\"\n    Return a JSON representation (of a list of dicts) of the messages in\n    the conversation. The keys of the dicts are the roles, the values are\n    the messages.\n\n    Returns:\n        str: A JSON representation of the messages in the conversation.\n    \"\"\"\n    d = []\n    for msg in self.messages:\n        if isinstance(msg, SystemMessage):\n            role = \"system\"\n        elif isinstance(msg, HumanMessage):\n            role = \"user\"\n        elif isinstance(msg, AIMessage):\n            role = \"ai\"\n        else:\n            raise ValueError(f\"Unknown message type: {type(msg)}\")\n\n        d.append({role: msg.content})\n\n    return json.dumps(d)\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.Conversation.setup","title":"<code>setup(context)</code>","text":"<p>Set up the conversation with general prompts and a context.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def setup(self, context: str):\n    \"\"\"\n    Set up the conversation with general prompts and a context.\n    \"\"\"\n    for msg in self.prompts[\"primary_model_prompts\"]:\n        if msg:\n            self.append_system_message(msg)\n\n    for msg in self.prompts[\"correcting_agent_prompts\"]:\n        if msg:\n            self.append_ca_message(msg)\n\n    self.context = context\n    msg = f\"The topic of the research is {context}.\"\n    self.append_system_message(msg)\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.GptConversation","title":"<code>GptConversation</code>","text":"<p>             Bases: <code>Conversation</code></p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class GptConversation(Conversation):\n    def __init__(\n        self,\n        model_name: str,\n        prompts: dict,\n        correct: bool = True,\n        split_correction: bool = False,\n        rag_agent: DocumentEmbedder = None,\n    ):\n        \"\"\"\n        Connect to OpenAI's GPT API and set up a conversation with the user.\n        Also initialise a second conversational agent to provide corrections to\n        the model output, if necessary.\n\n        Args:\n            model_name (str): The name of the model to use.\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            split_correction (bool): Whether to correct the model output by\n                splitting the output into sentences and correcting each\n                sentence individually.\n\n            rag_agent (DocumentEmbedder): A RAG agent to use for\n                retrieval augmented generation (RAG).\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n            rag_agent=rag_agent,\n        )\n\n        self.ca_model_name = \"gpt-3.5-turbo\"\n        # TODO make accessible by drop-down\n\n    def set_api_key(self, api_key: str, user: str):\n        \"\"\"\n        Set the API key for the OpenAI API. If the key is valid, initialise the\n        conversational agent. Set the user for usage statistics.\n\n        Args:\n            api_key (str): The API key for the OpenAI API.\n\n            user (str): The user for usage statistics.\n\n        Returns:\n            bool: True if the API key is valid, False otherwise.\n        \"\"\"\n        client = openai.OpenAI(\n            api_key=api_key,\n        )\n        self.user = user\n\n        try:\n            client.models.list()\n            self.chat = ChatOpenAI(\n                model_name=self.model_name,\n                temperature=0,\n                openai_api_key=api_key,\n            )\n            self.ca_chat = ChatOpenAI(\n                model_name=self.ca_model_name,\n                temperature=0,\n                openai_api_key=api_key,\n            )\n            if user == \"community\":\n                self.usage_stats = get_stats(user=user)\n\n            return True\n\n        except openai._exceptions.AuthenticationError as e:\n            return False\n\n    def _primary_query(self):\n        \"\"\"\n        Query the OpenAI API with the user's message and return the response\n        using the message history (flattery system messages, prior conversation)\n        as context. Correct the response if necessary.\n\n        Returns:\n            tuple: A tuple containing the response from the OpenAI API and the\n                token usage.\n        \"\"\"\n        try:\n            response = self.chat.generate([self.messages])\n        except (\n            openai._exceptions.APIError,\n            openai._exceptions.OpenAIError,\n            openai._exceptions.ConflictError,\n            openai._exceptions.NotFoundError,\n            openai._exceptions.APIStatusError,\n            openai._exceptions.RateLimitError,\n            openai._exceptions.APITimeoutError,\n            openai._exceptions.BadRequestError,\n            openai._exceptions.APIConnectionError,\n            openai._exceptions.AuthenticationError,\n            openai._exceptions.InternalServerError,\n            openai._exceptions.PermissionDeniedError,\n            openai._exceptions.UnprocessableEntityError,\n            openai._exceptions.APIResponseValidationError,\n        ) as e:\n            return str(e), None\n\n        msg = response.generations[0][0].text\n        token_usage = response.llm_output.get(\"token_usage\")\n\n        self._update_usage_stats(self.model_name, token_usage)\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _correct_response(self, msg: str):\n        \"\"\"\n        Correct the response from the OpenAI API by sending it to a secondary\n        language model. Optionally split the response into single sentences and\n        correct each sentence individually. Update usage stats.\n\n        Args:\n            msg (str): The response from the OpenAI API.\n\n        Returns:\n            str: The corrected response (or OK if no correction necessary).\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond \"\n                \"with just 'OK', and nothing else!\",\n            ),\n        )\n\n        response = self.ca_chat.generate([ca_messages])\n\n        correction = response.generations[0][0].text\n        token_usage = response.llm_output.get(\"token_usage\")\n\n        self._update_usage_stats(self.ca_model_name, token_usage)\n\n        return correction\n\n    def _update_usage_stats(self, model: str, token_usage: dict):\n        \"\"\"\n        Update redis database with token usage statistics using the usage_stats\n        object with the increment method.\n\n        Args:\n            model (str): The model name.\n\n            token_usage (dict): The token usage statistics.\n        \"\"\"\n        if self.user == \"community\":\n            self.usage_stats.increment(\n                f\"usage:[date]:[user]\",\n                {f\"{k}:{model}\": v for k, v in token_usage.items()},\n            )\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.GptConversation.__init__","title":"<code>__init__(model_name, prompts, correct=True, split_correction=False, rag_agent=None)</code>","text":"<p>Connect to OpenAI's GPT API and set up a conversation with the user. Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use.</p> required <code>prompts</code> <code>dict</code> <p>A dictionary of prompts to use for the conversation.</p> required <code>split_correction</code> <code>bool</code> <p>Whether to correct the model output by splitting the output into sentences and correcting each sentence individually.</p> <code>False</code> <code>rag_agent</code> <code>DocumentEmbedder</code> <p>A RAG agent to use for retrieval augmented generation (RAG).</p> <code>None</code> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    prompts: dict,\n    correct: bool = True,\n    split_correction: bool = False,\n    rag_agent: DocumentEmbedder = None,\n):\n    \"\"\"\n    Connect to OpenAI's GPT API and set up a conversation with the user.\n    Also initialise a second conversational agent to provide corrections to\n    the model output, if necessary.\n\n    Args:\n        model_name (str): The name of the model to use.\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each\n            sentence individually.\n\n        rag_agent (DocumentEmbedder): A RAG agent to use for\n            retrieval augmented generation (RAG).\n    \"\"\"\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n        rag_agent=rag_agent,\n    )\n\n    self.ca_model_name = \"gpt-3.5-turbo\"\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.GptConversation.set_api_key","title":"<code>set_api_key(api_key, user)</code>","text":"<p>Set the API key for the OpenAI API. If the key is valid, initialise the conversational agent. Set the user for usage statistics.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for the OpenAI API.</p> required <code>user</code> <code>str</code> <p>The user for usage statistics.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the API key is valid, False otherwise.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def set_api_key(self, api_key: str, user: str):\n    \"\"\"\n    Set the API key for the OpenAI API. If the key is valid, initialise the\n    conversational agent. Set the user for usage statistics.\n\n    Args:\n        api_key (str): The API key for the OpenAI API.\n\n        user (str): The user for usage statistics.\n\n    Returns:\n        bool: True if the API key is valid, False otherwise.\n    \"\"\"\n    client = openai.OpenAI(\n        api_key=api_key,\n    )\n    self.user = user\n\n    try:\n        client.models.list()\n        self.chat = ChatOpenAI(\n            model_name=self.model_name,\n            temperature=0,\n            openai_api_key=api_key,\n        )\n        self.ca_chat = ChatOpenAI(\n            model_name=self.ca_model_name,\n            temperature=0,\n            openai_api_key=api_key,\n        )\n        if user == \"community\":\n            self.usage_stats = get_stats(user=user)\n\n        return True\n\n    except openai._exceptions.AuthenticationError as e:\n        return False\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.XinferenceConversation","title":"<code>XinferenceConversation</code>","text":"<p>             Bases: <code>Conversation</code></p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>class XinferenceConversation(Conversation):\n    def __init__(\n        self,\n        base_url: str,\n        prompts: dict,\n        model_name: str = \"auto\",\n        correct: bool = True,\n        split_correction: bool = False,\n        rag_agent: DocumentEmbedder = None,\n    ):\n        \"\"\"\n\n        Connect to an open-source LLM via the Xinference client library and set\n        up a conversation with the user.  Also initialise a second\n        conversational agent to provide corrections to the model output, if\n        necessary.\n\n        Args:\n\n            base_url (str): The base URL of the Xinference instance (should not\n            include the /v1 part).\n\n            prompts (dict): A dictionary of prompts to use for the conversation.\n\n            model_name (str): The name of the model to use. Will be mapped to\n            the according uid from the list of available models. Can be set to\n            \"auto\" to use the first available model.\n\n            correct (bool): Whether to correct the model output.\n\n            split_correction (bool): Whether to correct the model output by\n            splitting the output into sentences and correcting each sentence\n            individually.\n\n            rag_agent (DocumentEmbedder): A RAG agent to use for retieval\n            augmented generation.\n\n        \"\"\"\n\n        super().__init__(\n            model_name=model_name,\n            prompts=prompts,\n            correct=correct,\n            split_correction=split_correction,\n            rag_agent=rag_agent,\n        )\n        self.client = Client(base_url=base_url)\n\n        self.models = {}\n        self.load_models()\n\n        self.ca_model_name = model_name\n\n        self.set_api_key()\n\n        # TODO make accessible by drop-down\n\n    def load_models(self):\n        for id, model in self.client.list_models().items():\n            model[\"id\"] = id\n            self.models[model[\"model_name\"]] = model\n\n    # def list_models_by_type(self, type: str):\n    #     names = []\n    #     if type == 'embed' or type == 'embedding':\n    #         for name, model in self.models.items():\n    #             if \"model_ability\" in model:\n    #                 if \"embed\" in model[\"model_ability\"]:\n    #                     names.append(name)\n    #             elif model[\"model_type\"] == \"embedding\":\n    #                 names.append(name)\n    #         return names\n    #     for name, model in self.models.items():\n    #         if \"model_ability\" in model:\n    #             if type in model[\"model_ability\"]:\n    #                 names.append(name)\n    #         elif model[\"model_type\"] == type:\n    #             names.append(name)\n    #     return names\n\n    def append_system_message(self, message: str):\n        \"\"\"\n        We override the system message addition because Xinference does not\n        accept multiple system messages. We concatenate them if there are\n        multiple.\n\n        Args:\n            message (str): The message to append.\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.messages):\n            self.messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.messages):\n                if isinstance(msg, SystemMessage):\n                    self.messages[i].content += f\"\\n{message}\"\n                    break\n\n    def append_ca_message(self, message: str):\n        \"\"\"\n\n        We also override the system message addition for the correcting agent,\n        likewise because Xinference does not accept multiple system messages. We\n        concatenate them if there are multiple.\n\n        TODO this currently assumes that the correcting agent is the same model\n        as the primary one.\n\n        Args:\n            message (str): The message to append.\n        \"\"\"\n        # if there is not already a system message in self.messages\n        if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n            self.ca_messages.append(\n                SystemMessage(\n                    content=message,\n                ),\n            )\n        else:\n            # if there is a system message, append to the last one\n            for i, msg in enumerate(self.ca_messages):\n                if isinstance(msg, SystemMessage):\n                    self.ca_messages[i].content += f\"\\n{message}\"\n                    break\n\n    def _primary_query(self):\n        \"\"\"\n\n        Query the Xinference client API with the user's message and return the\n        response using the message history (flattery system messages, prior\n        conversation) as context. Correct the response if necessary.\n\n        Returns:\n\n            tuple: A tuple containing the response from the Xinference API\n            (formatted similarly to responses from the OpenAI API) and the token\n            usage.\n\n        \"\"\"\n        try:\n            history = []\n            for m in self.messages:\n                if isinstance(m, SystemMessage):\n                    history.append({\"role\": \"system\", \"content\": m.content})\n                elif isinstance(m, HumanMessage):\n                    history.append({\"role\": \"user\", \"content\": m.content})\n                elif isinstance(m, AIMessage):\n                    history.append({\"role\": \"assistant\", \"content\": m.content})\n            prompt = history.pop()\n            response = self.model.chat(\n                prompt=prompt[\"content\"],\n                chat_history=history,\n                generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n            )\n        except (\n            openai._exceptions.APIError,\n            openai._exceptions.OpenAIError,\n            openai._exceptions.ConflictError,\n            openai._exceptions.NotFoundError,\n            openai._exceptions.APIStatusError,\n            openai._exceptions.RateLimitError,\n            openai._exceptions.APITimeoutError,\n            openai._exceptions.BadRequestError,\n            openai._exceptions.APIConnectionError,\n            openai._exceptions.AuthenticationError,\n            openai._exceptions.InternalServerError,\n            openai._exceptions.PermissionDeniedError,\n            openai._exceptions.UnprocessableEntityError,\n            openai._exceptions.APIResponseValidationError,\n        ) as e:\n            return str(e), None\n\n        msg = response[\"choices\"][0][\"message\"][\"content\"]\n        token_usage = response[\"usage\"]\n\n        self._update_usage_stats(self.model_name, token_usage)\n\n        self.append_ai_message(msg)\n\n        return msg, token_usage\n\n    def _correct_response(self, msg: str):\n        \"\"\"\n\n        Correct the response from the Xinference API by sending it to a\n        secondary language model. Optionally split the response into single\n        sentences and correct each sentence individually. Update usage stats.\n\n        Args:\n            msg (str): The response from the model.\n\n        Returns:\n            str: The corrected response (or OK if no correction necessary).\n        \"\"\"\n        ca_messages = self.ca_messages.copy()\n        ca_messages.append(\n            HumanMessage(\n                content=msg,\n            ),\n        )\n        ca_messages.append(\n            SystemMessage(\n                content=\"If there is nothing to correct, please respond \"\n                \"with just 'OK', and nothing else!\",\n            ),\n        )\n        history = []\n        for m in self.messages:\n            if isinstance(m, SystemMessage):\n                history.append({\"role\": \"system\", \"content\": m.content})\n            elif isinstance(m, HumanMessage):\n                history.append({\"role\": \"user\", \"content\": m.content})\n            elif isinstance(m, AIMessage):\n                history.append({\"role\": \"assistant\", \"content\": m.content})\n        prompt = history.pop()\n        response = self.ca_model.chat(\n            prompt=prompt[\"content\"],\n            chat_history=history,\n            generate_config={\"max_tokens\": 2048, \"temperature\": 0},\n        )\n\n        correction = response[\"choices\"][0][\"message\"][\"content\"]\n        token_usage = response[\"usage\"]\n\n        self._update_usage_stats(self.ca_model_name, token_usage)\n\n        return correction\n\n    def _update_usage_stats(self, model: str, token_usage: dict):\n        \"\"\"\n        Update redis database with token usage statistics using the usage_stats\n        object with the increment method.\n\n        Args:\n            model (str): The model name.\n\n            token_usage (dict): The token usage statistics.\n        \"\"\"\n        # if self.user == \"community\":\n        # self.usage_stats.increment(\n        #     f\"usage:[date]:[user]\",\n        #     {f\"{k}:{model}\": v for k, v in token_usage.items()},\n        # )\n\n    def set_api_key(self):\n        \"\"\"\n        Try to get the Xinference model from the client API. If the model is\n        found, initialise the conversational agent. If the model is not found,\n        `get_model` will raise a RuntimeError.\n\n        Returns:\n            bool: True if the model is found, False otherwise.\n        \"\"\"\n\n        try:\n            if self.model_name is None or self.model_name == \"auto\":\n                self.model_name = self.list_models_by_type(\"chat\")[0]\n            self.model = self.client.get_model(\n                self.models[self.model_name][\"id\"]\n            )\n\n            if self.ca_model_name is None or self.ca_model_name == \"auto\":\n                self.ca_model_name = self.list_models_by_type(\"chat\")[0]\n            self.ca_model = self.client.get_model(\n                self.models[self.ca_model_name][\"id\"]\n            )\n            return True\n\n        except RuntimeError as e:\n            # TODO handle error, log?\n            return False\n\n    def list_models_by_type(self, type: str):\n        names = []\n        if type == \"embed\" or type == \"embedding\":\n            for name, model in self.models.items():\n                if \"model_ability\" in model:\n                    if \"embed\" in model[\"model_ability\"]:\n                        names.append(name)\n                elif model[\"model_type\"] == \"embedding\":\n                    names.append(name)\n            return names\n        for name, model in self.models.items():\n            if \"model_ability\" in model:\n                if type in model[\"model_ability\"]:\n                    names.append(name)\n            elif model[\"model_type\"] == type:\n                names.append(name)\n        return names\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.XinferenceConversation.__init__","title":"<code>__init__(base_url, prompts, model_name='auto', correct=True, split_correction=False, rag_agent=None)</code>","text":"<p>Connect to an open-source LLM via the Xinference client library and set up a conversation with the user.  Also initialise a second conversational agent to provide corrections to the model output, if necessary.</p> <p>Args:</p> <pre><code>base_url (str): The base URL of the Xinference instance (should not\ninclude the /v1 part).\n\nprompts (dict): A dictionary of prompts to use for the conversation.\n\nmodel_name (str): The name of the model to use. Will be mapped to\nthe according uid from the list of available models. Can be set to\n\"auto\" to use the first available model.\n\ncorrect (bool): Whether to correct the model output.\n\nsplit_correction (bool): Whether to correct the model output by\nsplitting the output into sentences and correcting each sentence\nindividually.\n\nrag_agent (DocumentEmbedder): A RAG agent to use for retieval\naugmented generation.\n</code></pre> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def __init__(\n    self,\n    base_url: str,\n    prompts: dict,\n    model_name: str = \"auto\",\n    correct: bool = True,\n    split_correction: bool = False,\n    rag_agent: DocumentEmbedder = None,\n):\n    \"\"\"\n\n    Connect to an open-source LLM via the Xinference client library and set\n    up a conversation with the user.  Also initialise a second\n    conversational agent to provide corrections to the model output, if\n    necessary.\n\n    Args:\n\n        base_url (str): The base URL of the Xinference instance (should not\n        include the /v1 part).\n\n        prompts (dict): A dictionary of prompts to use for the conversation.\n\n        model_name (str): The name of the model to use. Will be mapped to\n        the according uid from the list of available models. Can be set to\n        \"auto\" to use the first available model.\n\n        correct (bool): Whether to correct the model output.\n\n        split_correction (bool): Whether to correct the model output by\n        splitting the output into sentences and correcting each sentence\n        individually.\n\n        rag_agent (DocumentEmbedder): A RAG agent to use for retieval\n        augmented generation.\n\n    \"\"\"\n\n    super().__init__(\n        model_name=model_name,\n        prompts=prompts,\n        correct=correct,\n        split_correction=split_correction,\n        rag_agent=rag_agent,\n    )\n    self.client = Client(base_url=base_url)\n\n    self.models = {}\n    self.load_models()\n\n    self.ca_model_name = model_name\n\n    self.set_api_key()\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.XinferenceConversation.append_ca_message","title":"<code>append_ca_message(message)</code>","text":"<p>We also override the system message addition for the correcting agent, likewise because Xinference does not accept multiple system messages. We concatenate them if there are multiple.</p> <p>TODO this currently assumes that the correcting agent is the same model as the primary one.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to append.</p> required Source code in <code>biochatter/llm_connect.py</code> <pre><code>def append_ca_message(self, message: str):\n    \"\"\"\n\n    We also override the system message addition for the correcting agent,\n    likewise because Xinference does not accept multiple system messages. We\n    concatenate them if there are multiple.\n\n    TODO this currently assumes that the correcting agent is the same model\n    as the primary one.\n\n    Args:\n        message (str): The message to append.\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.ca_messages):\n        self.ca_messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.ca_messages):\n            if isinstance(msg, SystemMessage):\n                self.ca_messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.XinferenceConversation.append_system_message","title":"<code>append_system_message(message)</code>","text":"<p>We override the system message addition because Xinference does not accept multiple system messages. We concatenate them if there are multiple.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to append.</p> required Source code in <code>biochatter/llm_connect.py</code> <pre><code>def append_system_message(self, message: str):\n    \"\"\"\n    We override the system message addition because Xinference does not\n    accept multiple system messages. We concatenate them if there are\n    multiple.\n\n    Args:\n        message (str): The message to append.\n    \"\"\"\n    # if there is not already a system message in self.messages\n    if not any(isinstance(m, SystemMessage) for m in self.messages):\n        self.messages.append(\n            SystemMessage(\n                content=message,\n            ),\n        )\n    else:\n        # if there is a system message, append to the last one\n        for i, msg in enumerate(self.messages):\n            if isinstance(msg, SystemMessage):\n                self.messages[i].content += f\"\\n{message}\"\n                break\n</code></pre>"},{"location":"llm_connect-reference/#biochatter.llm_connect.XinferenceConversation.set_api_key","title":"<code>set_api_key()</code>","text":"<p>Try to get the Xinference model from the client API. If the model is found, initialise the conversational agent. If the model is not found, <code>get_model</code> will raise a RuntimeError.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the model is found, False otherwise.</p> Source code in <code>biochatter/llm_connect.py</code> <pre><code>def set_api_key(self):\n    \"\"\"\n    Try to get the Xinference model from the client API. If the model is\n    found, initialise the conversational agent. If the model is not found,\n    `get_model` will raise a RuntimeError.\n\n    Returns:\n        bool: True if the model is found, False otherwise.\n    \"\"\"\n\n    try:\n        if self.model_name is None or self.model_name == \"auto\":\n            self.model_name = self.list_models_by_type(\"chat\")[0]\n        self.model = self.client.get_model(\n            self.models[self.model_name][\"id\"]\n        )\n\n        if self.ca_model_name is None or self.ca_model_name == \"auto\":\n            self.ca_model_name = self.list_models_by_type(\"chat\")[0]\n        self.ca_model = self.client.get_model(\n            self.models[self.ca_model_name][\"id\"]\n        )\n        return True\n\n    except RuntimeError as e:\n        # TODO handle error, log?\n        return False\n</code></pre>"},{"location":"open-llm/","title":"Open-source and Local LLMs","text":"<p>Xorbits Inference is an open-source toolkit for running open-source models, particularly language models. To support BioChatter applications in local and protected contexts, we provide API access through the LangChain OpenAI Xinference module. Briefly, this module allows to connect to any open-source model supported by Xinference via the state-of-the-art and easy-to-use OpenAI API. This allows local and remote access to essentially all relevant open-source models, including these builtin models, at very little setup cost.</p>"},{"location":"open-llm/#usage","title":"Usage","text":"<p>Usage is essentially the same as when calling the official OpenAI API, but uses the <code>XinferenceConversation</code> class under the hood. Interaction with the class is possible in the exact same way as with the standard class.</p>"},{"location":"open-llm/#connecting-to-the-model-from-biochatter","title":"Connecting to the model from BioChatter","text":"<p>All that remains once Xinference has started your model is to tell BioChatter the API endpoint of your deployed model via the <code>base_url</code> parameter of the <code>XinferenceConversation</code> class. For instance:</p> <pre><code>from biochatter.llm_connect import XinferenceConversation\n\nconversation = XinferenceConversation(\n         base_url=\"http://llm.biocypher.org\",\n         prompts={},\n         correct=False,\n     )\nresponse, token_usage, correction = conversation.query(\"Hello world!\")\n</code></pre>"},{"location":"open-llm/#deploying-locally-via-docker","title":"Deploying locally via Docker","text":"<p>We have created a Docker workflow that allows the deployment of builtin Xinference models, here. It will soon be available via Dockerhub. There is another workflow that allows mounting (potentially) any compatible model from HuggingFace, here.</p>"},{"location":"open-llm/#deploying-locally-without-docker","title":"Deploying locally without Docker","text":""},{"location":"open-llm/#installation","title":"Installation","text":"<p>To run Xinference locally on your computer or a workstation available on your network, follow the official instructions for your type of hardware. Briefly, this includes installing the <code>xinference</code> and <code>ctransformers</code> Python libraries into an environment of your choice, as well as a hardware-specific installation of the <code>llama-ccp-python</code> library.</p>"},{"location":"open-llm/#deploying-your-model","title":"Deploying your model","text":"<p>After installation, you can run the model (locally using <code>xinference</code> or in a distributed fashion. After startup, you can visit the local server address in your browser (standard is <code>http://localhost:9997</code>) and select and start your desired model. There is a large selection of predefined models to choose from, as well as the possibility to add your own favourite models to the framework. You will see your running models in the <code>Running Models</code> tab, once they have started.</p> <p>Alternatively, you can deploy (and query) your model via the Xinference Python client:</p> <pre><code>from xinference.client import Client\n\nclient = Client(\"http://localhost:9997\")\nmodel_uid = client.launch_model(model_name=\"chatglm2\")  # download model from HuggingFace and deploy\nmodel = client.get_model(model_uid)\n\nchat_history = []\nprompt = \"What is the largest animal?\"\nmodel.chat(\n    prompt,\n    chat_history,\n    generate_config={\"max_tokens\": 1024}\n)\n</code></pre>"},{"location":"open-llm/#public-endpoint","title":"Public endpoint","text":"<p>We maintain a public API endpoint of an xinference instance at <code>https://llm.biocypher.org/v1</code>. Please note that this is a testing endpoint that may be subject to rapid changes or downtimes. It usually runs one conversational and one embedding model.</p>"},{"location":"podcast-reference/","title":"Podcast module","text":"<p>Here we handle generation of podcasts from texts.</p>"},{"location":"podcast-reference/#biochatter.podcast.Podcaster","title":"<code>Podcaster</code>","text":"Source code in <code>biochatter/podcast.py</code> <pre><code>class Podcaster:\n    def __init__(\n        self,\n        document: Document,\n        model_name: str = \"gpt-3.5-turbo\",\n    ) -&gt; None:\n        \"\"\"\n        Orchestrates the podcasting of a document.\n        \"\"\"\n        self.document = document\n        self.model_name = model_name\n\n    def generate_podcast(self, characters_per_paragraph: int) -&gt; None:\n        \"\"\"\n        Podcasts the document.\n\n        TODO:\n        - chain of density prompting for variable summary length\n        \"\"\"\n        full_text = self.document[0].page_content\n\n        # split text by sentence\n        sentences = self._split_text(full_text)\n\n        # could embed sentences and cluster on cosine similarity to identify\n        # paragraphs here\n\n        # preprocess text\n        for i, sentence in enumerate(sentences):\n            # special cases i.e. and e.g. - if sentence ends with one of these,\n            # append next sentence\n            special_cases = [\"i.e.\", \"e.g.\"]\n            if sentence.endswith(tuple(special_cases)):\n                sentences[i] = sentence + \" \" + sentences[i + 1]\n                del sentences[i + 1]\n\n        # concatenate first 5 sentences for title and author extraction\n        first_5 = \"\\n\".join(sentences[:5])\n        self.podcast_info = self._title_and_authors(first_5)\n\n        # LLM to determine section breaks?\n\n        # go through sections and summarise each\n        self.processed_sections = self._process_sections(\n            sentences,\n            characters_per_paragraph,\n        )\n\n        # summarise the summaries\n\n    def _split_text(self, text: str) -&gt; List[str]:\n        \"\"\"\n        Splits consecutive text into sentences.\n        \"\"\"\n        nltk.download(\"punkt\")\n        tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n        return tokenizer.tokenize(text)\n\n    def _title_and_authors(self, text: str) -&gt; str:\n        \"\"\"\n        Extracts title and authors from document.\n\n        Args:\n            text (str): text to extract title and authors from\n\n        Returns:\n            str: title and authors\n        \"\"\"\n        # first sentence - extract title, authors\n        c_first = GptConversation(\n            model_name=self.model_name,\n            prompts={},\n            correct=False,\n        )\n        c_first.set_api_key(api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"podcast\")\n        c_first.append_system_message(FIRST_PROMPT)\n        msg, token_usage, correction = c_first.query(text)\n        # split at authors ('Authors:' or '\\nAuthors:')\n        if \"Authors:\" in msg:\n            title = msg.split(\"Title:\")[1].split(\"Authors:\")[0].strip()\n            authors = msg.split(\"Authors:\")[1].strip()\n            return f\"{title}, by {authors}, podcasted by biochatter.\"\n        else:\n            return \"A podcast by biochatter.\"\n\n    def _process_section(self, text: str, summarise: bool = False) -&gt; str:\n        \"\"\"\n        Processes a section of the document. Summarises if summarise is True,\n        otherwise just makes the text more listenable.\n\n        Args:\n            text (str): text to summarise\n\n            summarise (bool): whether to summarise the text\n\n        Returns:\n            str: summarised text\n        \"\"\"\n        # summarise section\n        c = GptConversation(\n            model_name=self.model_name,\n            prompts={},\n            correct=False,\n        )\n        c.set_api_key(api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"podcast\")\n        if summarise:\n            c.append_system_message(SUMMARISE_PROMPT)\n        else:\n            c.append_system_message(PROCESS_PROMPT)\n        msg, token_usage, correction = c.query(text)\n        return msg\n\n    def _process_sections(\n        self, sentences: list, characters_per_paragraph: int\n    ) -&gt; list:\n        \"\"\"\n\n        Processes sections of the document. Concatenates sentences until\n        characters_per_paragraph is reached, removing each sentence from the\n        list as it is added to the section to be processed.\n\n        Args:\n            sentences (list): list of sentences to summarise\n\n            characters_per_paragraph (int): number of characters per paragraph\n\n        Returns:\n            list: list of processed sections\n        \"\"\"\n        summarised_sections = []\n        section = \"\"\n        while sentences:\n            sentence = sentences.pop(0)\n            tmp = section + sentence\n            if len(tmp) &lt; characters_per_paragraph and sentences:\n                section += sentence\n            else:\n                if sentences:\n                    sentences.insert(0, sentence)\n                summarised_section = self._process_section(section)\n                # filter \"no content\" sections\n                if not (\n                    \"no content\" in summarised_section.lower()\n                    and len(summarised_section) &lt; 30\n                ):\n                    summarised_sections.append(summarised_section)\n                section = \"\"\n\n        return summarised_sections\n\n    def podcast_to_file(\n        self,\n        path: str,\n        model: str = \"gtts\",\n        voice: str = \"alloy\",\n    ) -&gt; None:\n        \"\"\"\n        Uses text-to-speech to generate audio for the summarised paper podcast.\n\n        Args:\n            path (str): path to save audio file to\n\n            model (str): model to use for text-to-speech. Currently supported:\n                'gtts' (Google Text-to-Speech, free),\n                'tts-1' (OpenAI API, paid, prioritises speed),\n                'tts-1-hd' (OpenAI API, paid, prioritises quality)\n\n            voice (str): voice to use for text-to-speech. See OpenAI API\n                documentation for available voices.\n        \"\"\"\n\n        full_text = self.podcast_to_text()\n\n        if model == \"gtts\":\n            audio = gTTS(text=full_text)\n            audio.save(path)\n        else:\n            client = OpenAI()\n\n            # Save the intro to the original file\n            response = client.audio.speech.create(\n                model=model,\n                voice=voice,\n                input=(\n                    \"You are listening to: \\n\\n\"\n                    + self.podcast_info\n                    + \"\\n\\n\"\n                    + \" Text-to-speech generated by OpenAI.\"\n                ),\n            )\n            first_path = path.rsplit(\".\", 1)[0] + \"_0.mp3\"\n            response.stream_to_file(first_path)\n\n            # Concatenate the sections\n            full_text = \"\"\n            for i, section in enumerate(self.processed_sections):\n                full_text += section + \"\\n\\n\"\n\n            # Make sections of 4000 characters max (at sentence boundaries)\n            nltk.download(\"punkt\")\n            tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n            sentences = deque(\n                tokenizer.tokenize(full_text)\n            )  # Use a deque instead of a list\n\n            # Split the text into sections by filling one section until it\n            # exceeds 4000 characters, then starting a new section (not adding\n            # the sentence that would exceed the limit)\n            sections = []\n            section = \"\"\n            while sentences:\n                sentence = sentences[0]\n                tmp = section + sentence\n                if len(tmp) &lt; 4000:\n                    section += sentences.popleft()\n                else:\n                    sections.append(section)\n                    section = \"\"\n\n            sections.append(section)  # Add the penultimate section\n\n            # Last section: conclude the podcast\n            sections.append(\n                f\"This was {self.podcast_info}. Thank you for listening.\"\n            )\n\n            # Save each section to a separate file with an integer suffix\n            for i, section in enumerate(sections):\n                response = client.audio.speech.create(\n                    model=model,\n                    voice=voice,\n                    input=section,\n                )\n                # Insert the integer suffix just before the .mp3 extension\n                section_path = path.rsplit(\".\", 1)[0] + f\"_{i+1}.mp3\"\n                response.stream_to_file(section_path)\n\n    def podcast_to_text(self):\n        \"\"\"\n        Returns the summarised paper podcast as text.\n        \"\"\"\n        full_text = \"You are listening to: \" + self.podcast_info + \"\\n\\n\"\n        for section in self.processed_sections:\n            full_text += section + \"\\n\\n\"\n        return full_text\n</code></pre>"},{"location":"podcast-reference/#biochatter.podcast.Podcaster.__init__","title":"<code>__init__(document, model_name='gpt-3.5-turbo')</code>","text":"<p>Orchestrates the podcasting of a document.</p> Source code in <code>biochatter/podcast.py</code> <pre><code>def __init__(\n    self,\n    document: Document,\n    model_name: str = \"gpt-3.5-turbo\",\n) -&gt; None:\n    \"\"\"\n    Orchestrates the podcasting of a document.\n    \"\"\"\n    self.document = document\n    self.model_name = model_name\n</code></pre>"},{"location":"podcast-reference/#biochatter.podcast.Podcaster.generate_podcast","title":"<code>generate_podcast(characters_per_paragraph)</code>","text":"<p>Podcasts the document.</p> <p>TODO: - chain of density prompting for variable summary length</p> Source code in <code>biochatter/podcast.py</code> <pre><code>def generate_podcast(self, characters_per_paragraph: int) -&gt; None:\n    \"\"\"\n    Podcasts the document.\n\n    TODO:\n    - chain of density prompting for variable summary length\n    \"\"\"\n    full_text = self.document[0].page_content\n\n    # split text by sentence\n    sentences = self._split_text(full_text)\n\n    # could embed sentences and cluster on cosine similarity to identify\n    # paragraphs here\n\n    # preprocess text\n    for i, sentence in enumerate(sentences):\n        # special cases i.e. and e.g. - if sentence ends with one of these,\n        # append next sentence\n        special_cases = [\"i.e.\", \"e.g.\"]\n        if sentence.endswith(tuple(special_cases)):\n            sentences[i] = sentence + \" \" + sentences[i + 1]\n            del sentences[i + 1]\n\n    # concatenate first 5 sentences for title and author extraction\n    first_5 = \"\\n\".join(sentences[:5])\n    self.podcast_info = self._title_and_authors(first_5)\n\n    # LLM to determine section breaks?\n\n    # go through sections and summarise each\n    self.processed_sections = self._process_sections(\n        sentences,\n        characters_per_paragraph,\n    )\n</code></pre>"},{"location":"podcast-reference/#biochatter.podcast.Podcaster.podcast_to_file","title":"<code>podcast_to_file(path, model='gtts', voice='alloy')</code>","text":"<p>Uses text-to-speech to generate audio for the summarised paper podcast.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to save audio file to</p> required <code>model</code> <code>str</code> <p>model to use for text-to-speech. Currently supported: 'gtts' (Google Text-to-Speech, free), 'tts-1' (OpenAI API, paid, prioritises speed), 'tts-1-hd' (OpenAI API, paid, prioritises quality)</p> <code>'gtts'</code> <code>voice</code> <code>str</code> <p>voice to use for text-to-speech. See OpenAI API documentation for available voices.</p> <code>'alloy'</code> Source code in <code>biochatter/podcast.py</code> <pre><code>def podcast_to_file(\n    self,\n    path: str,\n    model: str = \"gtts\",\n    voice: str = \"alloy\",\n) -&gt; None:\n    \"\"\"\n    Uses text-to-speech to generate audio for the summarised paper podcast.\n\n    Args:\n        path (str): path to save audio file to\n\n        model (str): model to use for text-to-speech. Currently supported:\n            'gtts' (Google Text-to-Speech, free),\n            'tts-1' (OpenAI API, paid, prioritises speed),\n            'tts-1-hd' (OpenAI API, paid, prioritises quality)\n\n        voice (str): voice to use for text-to-speech. See OpenAI API\n            documentation for available voices.\n    \"\"\"\n\n    full_text = self.podcast_to_text()\n\n    if model == \"gtts\":\n        audio = gTTS(text=full_text)\n        audio.save(path)\n    else:\n        client = OpenAI()\n\n        # Save the intro to the original file\n        response = client.audio.speech.create(\n            model=model,\n            voice=voice,\n            input=(\n                \"You are listening to: \\n\\n\"\n                + self.podcast_info\n                + \"\\n\\n\"\n                + \" Text-to-speech generated by OpenAI.\"\n            ),\n        )\n        first_path = path.rsplit(\".\", 1)[0] + \"_0.mp3\"\n        response.stream_to_file(first_path)\n\n        # Concatenate the sections\n        full_text = \"\"\n        for i, section in enumerate(self.processed_sections):\n            full_text += section + \"\\n\\n\"\n\n        # Make sections of 4000 characters max (at sentence boundaries)\n        nltk.download(\"punkt\")\n        tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n        sentences = deque(\n            tokenizer.tokenize(full_text)\n        )  # Use a deque instead of a list\n\n        # Split the text into sections by filling one section until it\n        # exceeds 4000 characters, then starting a new section (not adding\n        # the sentence that would exceed the limit)\n        sections = []\n        section = \"\"\n        while sentences:\n            sentence = sentences[0]\n            tmp = section + sentence\n            if len(tmp) &lt; 4000:\n                section += sentences.popleft()\n            else:\n                sections.append(section)\n                section = \"\"\n\n        sections.append(section)  # Add the penultimate section\n\n        # Last section: conclude the podcast\n        sections.append(\n            f\"This was {self.podcast_info}. Thank you for listening.\"\n        )\n\n        # Save each section to a separate file with an integer suffix\n        for i, section in enumerate(sections):\n            response = client.audio.speech.create(\n                model=model,\n                voice=voice,\n                input=section,\n            )\n            # Insert the integer suffix just before the .mp3 extension\n            section_path = path.rsplit(\".\", 1)[0] + f\"_{i+1}.mp3\"\n            response.stream_to_file(section_path)\n</code></pre>"},{"location":"podcast-reference/#biochatter.podcast.Podcaster.podcast_to_text","title":"<code>podcast_to_text()</code>","text":"<p>Returns the summarised paper podcast as text.</p> Source code in <code>biochatter/podcast.py</code> <pre><code>def podcast_to_text(self):\n    \"\"\"\n    Returns the summarised paper podcast as text.\n    \"\"\"\n    full_text = \"You are listening to: \" + self.podcast_info + \"\\n\\n\"\n    for section in self.processed_sections:\n        full_text += section + \"\\n\\n\"\n    return full_text\n</code></pre>"},{"location":"podcast/","title":"Podcast my Paper","text":"<p>We provide a module to perform document processing and text-to-speech to enable listening to any document in podcast style.  The functionality can be accessed through the podcast API or by running the script <code>scripts/podcast_single_document.py</code>.</p>"},{"location":"podcast/#api-access","title":"API access","text":"<p>The podcast API is available through the <code>podcast</code> module. An end-to-end workflow looks like this (modified from the test module):</p> <pre><code>from biochatter.podcast import Podcaster\nfrom biochatter.vectorstore import DocumentReader\n\n# Load document\nreader = DocumentReader()\ndocument = reader.load_document(\"test/dcn.pdf\")\n\n# Initialise podcaster\npodcaster = Podcaster(document)\n\n# Generate podcast (LLM task)\npodcaster.generate_podcast(characters_per_paragraph=5000)\n\n# Employ text-to-speech to generate audio file (optional)\npodcaster.podcast_to_file(\"test/test.mp3\", model=\"tts-1-hd\", voice=\"alloy\")\n</code></pre> <p>If you do not want audio output, you can simply access the generated text though the function <code>podcaster.podcast_to_text()</code>. </p> <p>This example uses the paid OpenAI text-to-speech API to generate the audio file. The default of the <code>podcast_to_file</code> function is to use the free Google text-to-speech API.  When using OpenAI, due to the input length limit of 4096 characters, the podcast is split into multiple parts indicated by integer suffixes.</p>"},{"location":"podcast/#command-line-access","title":"Command line access","text":"<p>To generate a podcast from a single document more quickly, you can use the <code>scripts/podcast_single_document.py</code> script.  It accepts two arguments, the path to the document and the path to the desired output file.  If the output file ends in <code>.mp3</code>, the OpenAI text-to-speech API will be used to generate an audio file.  Otherwise, the script will generate a text file and skip the text-to-speech step.  If using the OpenAI text-to-speech API, multiple files will be generated with integer suffixes.  If you installed BioChatter with poetry, you can run the script like this (from the root directory of the repository):</p> <pre><code>poetry run python scripts/podcast_single_document.py test/dcn.pdf test/test.mp3\n</code></pre>"},{"location":"prompts-reference/","title":"Prompts module","text":"<p>Here we handle generation of use case-specific prompts.</p>"},{"location":"prompts-reference/#biochatter.prompts.BioCypherPromptEngine","title":"<code>BioCypherPromptEngine</code>","text":"Source code in <code>biochatter/prompts.py</code> <pre><code>class BioCypherPromptEngine:\n    def __init__(\n        self,\n        schema_config_or_info_path: Optional[str] = None,\n        schema_config_or_info_dict: Optional[dict] = None,\n        model_name: str = \"gpt-3.5-turbo\",\n    ):\n        \"\"\"\n\n        Given a biocypher schema configuration, extract the entities and\n        relationships, and for each extract their mode of representation (node\n        or edge), properties, and identifier namespace. Using these data, allow\n        the generation of prompts for a large language model, informing it of\n        the schema constituents and their properties, to enable the\n        parameterisation of function calls to a knowledge graph.\n\n        Args:\n            schema_config_or_info_path: Path to a biocypher schema configuration\n                file or the extended schema information output generated by\n                BioCypher's `write_schema_info` function (preferred).\n\n            schema_config_or_info_dict: A dictionary containing the schema\n                configuration file or the extended schema information output\n                generated by BioCypher's `write_schema_info` function\n                (preferred).\n\n        Todo:\n            inject conversation directly instead of specifying model name?\n        \"\"\"\n\n        if not schema_config_or_info_path and not schema_config_or_info_dict:\n            raise ValueError(\n                \"Please provide the schema configuration or schema info as a \"\n                \"path to a file or as a dictionary.\"\n            )\n\n        if schema_config_or_info_path and schema_config_or_info_dict:\n            raise ValueError(\n                \"Please provide the schema configuration or schema info as a \"\n                \"path to a file or as a dictionary, not both.\"\n            )\n\n        if schema_config_or_info_path:\n            # read the schema configuration\n            with open(schema_config_or_info_path, \"r\") as f:\n                schema_config = yaml.safe_load(f)\n        elif schema_config_or_info_dict:\n            schema_config = schema_config_or_info_dict\n\n        # check whether it is the original schema config or the output of\n        # biocypher info\n        is_schema_info = schema_config.get(\"is_schema_info\", False)\n\n        # extract the entities and relationships: each top level key that has\n        # a 'represented_as' key\n        self.entities = {}\n        self.relationships = {}\n        if not is_schema_info:\n            for key, value in schema_config.items():\n                # hacky, better with biocypher output\n                name_indicates_relationship = (\n                    \"interaction\" in key.lower() or \"association\" in key.lower()\n                )\n                if \"represented_as\" in value:\n                    if (\n                        value[\"represented_as\"] == \"node\"\n                        and not name_indicates_relationship\n                    ):\n                        self.entities[sentencecase_to_pascalcase(key)] = value\n                    elif (\n                        value[\"represented_as\"] == \"node\"\n                        and name_indicates_relationship\n                    ):\n                        self.relationships[\n                            sentencecase_to_pascalcase(key)\n                        ] = value\n                    elif value[\"represented_as\"] == \"edge\":\n                        self.relationships[\n                            sentencecase_to_pascalcase(key)\n                        ] = value\n        else:\n            for key, value in schema_config.items():\n                if not isinstance(value, dict):\n                    continue\n                if value.get(\"present_in_knowledge_graph\", None) == False:\n                    continue\n                if value.get(\"is_relationship\", None) == False:\n                    self.entities[sentencecase_to_pascalcase(key)] = value\n                elif value.get(\"is_relationship\", None) == True:\n                    value = self._capitalise_source_and_target(value)\n                    self.relationships[sentencecase_to_pascalcase(key)] = value\n\n        self.question = \"\"\n        self.selected_entities = []\n        self.selected_relationships = []  # used in property selection\n        self.selected_relationship_labels = {}  # copy to deal with labels that\n        # are not the same as the relationship name, used in query generation\n        # dictionary to also include source and target types\n        self.rel_directions = {}\n        self.model_name = model_name\n\n    def _capitalise_source_and_target(self, relationship: dict) -&gt; dict:\n        \"\"\"\n        Make sources and targets PascalCase to match the entities. Sources and\n        targets can be strings or lists of strings.\n        \"\"\"\n        if \"source\" in relationship:\n            if isinstance(relationship[\"source\"], str):\n                relationship[\"source\"] = sentencecase_to_pascalcase(\n                    relationship[\"source\"]\n                )\n            elif isinstance(relationship[\"source\"], list):\n                relationship[\"source\"] = [\n                    sentencecase_to_pascalcase(s)\n                    for s in relationship[\"source\"]\n                ]\n        if \"target\" in relationship:\n            if isinstance(relationship[\"target\"], str):\n                relationship[\"target\"] = sentencecase_to_pascalcase(\n                    relationship[\"target\"]\n                )\n            elif isinstance(relationship[\"target\"], list):\n                relationship[\"target\"] = [\n                    sentencecase_to_pascalcase(t)\n                    for t in relationship[\"target\"]\n                ]\n        return relationship\n\n    def generate_query(self, question: str, query_language: str) -&gt; str:\n        \"\"\"\n        Wrap entity and property selection and query generation; return the\n        generated query.\n\n        Args:\n            question: A user's question.\n\n            query_language: The language of the query to generate.\n\n        Returns:\n            A database query that could answer the user's question.\n        \"\"\"\n\n        success1 = self._select_entities(question)\n        if not success1:\n            raise ValueError(\n                \"Entity selection failed. Please try again with a different \"\n                \"question.\"\n            )\n        success2 = self._select_relationships()\n        if not success2:\n            raise ValueError(\n                \"Relationship selection failed. Please try again with a \"\n                \"different question.\"\n            )\n        success3 = self._select_properties()\n        if not success3:\n            raise ValueError(\n                \"Property selection failed. Please try again with a different \"\n                \"question.\"\n            )\n\n        return self._generate_query(\n            question=question,\n            entities=self.selected_entities,\n            relationships=self.selected_relationship_labels,\n            properties=self.selected_properties,\n            query_language=query_language,\n        )\n\n    def _select_entities(self, question: str) -&gt; bool:\n        \"\"\"\n\n        Given a question, select the entities that are relevant to the question\n        and store them in `selected_entities` and `selected_relationships`. Use\n        LLM conversation to do this.\n\n        Args:\n            question: A user's question.\n\n        Returns:\n            True if at least one entity was selected, False otherwise.\n\n        \"\"\"\n\n        self.question = question\n\n        conversation = GptConversation(\n            model_name=self.model_name,\n            prompts={},\n            correct=False,\n        )\n\n        conversation.set_api_key(\n            api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"entity_selector\"\n        )\n\n        conversation.append_system_message(\n            (\n                \"You have access to a knowledge graph that contains \"\n                f\"these entities: {', '.join(self.entities)}. Your task is \"\n                \"to select the ones that are relevant to the user's question \"\n                \"for subsequent use in a query. Only return the entities, \"\n                \"comma-separated, without any additional text. \"\n            )\n        )\n\n        msg, token_usage, correction = conversation.query(question)\n\n        result = msg.split(\",\") if msg else []\n        # TODO: do we go back and retry if no entities were selected? or ask for\n        # a reason? offer visual selection of entities and relationships by the\n        # user?\n\n        if result:\n            for entity in result:\n                entity = entity.strip()\n                if entity in self.entities:\n                    self.selected_entities.append(entity)\n\n        return bool(result)\n\n    def _select_relationships(self) -&gt; bool:\n        \"\"\"\n        Given a question and the preselected entities, select relationships for\n        the query.\n\n        Args:\n            question: A user's question.\n\n            entities: A list of entities that are relevant to the question.\n\n        Returns:\n            True if at least one relationship was selected, False otherwise.\n\n        Todo:\n            Now we have the problem that we discard all relationships that do\n            not have a source and target, if at least one relationship has a\n            source and target. At least communicate this all-or-nothing\n            behaviour to the user.\n        \"\"\"\n\n        if not self.question:\n            raise ValueError(\n                \"No question found. Please make sure to run entity selection \"\n                \"first.\"\n            )\n\n        if not self.selected_entities:\n            raise ValueError(\n                \"No entities found. Please run the entity selection step first.\"\n            )\n\n        rels = {}\n        source_and_target_present = False\n        for key, value in self.relationships.items():\n            if \"source\" in value and \"target\" in value:\n                # if source or target is a list, expand to single pairs\n                source = ensure_iterable(value[\"source\"])\n                target = ensure_iterable(value[\"target\"])\n                pairs = []\n                for s in source:\n                    for t in target:\n                        pairs.append((s, t))\n                rels[key] = pairs\n                source_and_target_present = True\n            else:\n                rels[key] = {}\n\n        # prioritise relationships that have source and target, and discard\n        # relationships that do not have both source and target, if at least one\n        # relationship has both source and target. keep relationships that have\n        # either source or target, if none of the relationships have both source\n        # and target.\n\n        if source_and_target_present:\n            # First, separate the relationships into two groups: those with both\n            # source and target in the selected entities, and those with either\n            # source or target but not both.\n\n            rels_with_both = {}\n            rels_with_either = {}\n            for key, value in rels.items():\n                for pair in value:\n                    if pair[0] in self.selected_entities:\n                        if pair[1] in self.selected_entities:\n                            rels_with_both[key] = value\n                        else:\n                            rels_with_either[key] = value\n                    elif pair[1] in self.selected_entities:\n                        rels_with_either[key] = value\n\n            # If there are any relationships with both source and target,\n            # discard the others.\n\n            if rels_with_both:\n                rels = rels_with_both\n            else:\n                rels = rels_with_either\n\n            selected_rels = []\n            for key, value in rels.items():\n                if not value:\n                    continue\n\n                for pair in value:\n                    if (\n                        pair[0] in self.selected_entities\n                        or pair[1] in self.selected_entities\n                    ):\n                        selected_rels.append((key, pair))\n\n            rels = json.dumps(selected_rels)\n        else:\n            rels = json.dumps(self.relationships)\n\n        conversation = GptConversation(\n            model_name=self.model_name,\n            prompts={},\n            correct=False,\n        )\n\n        conversation.set_api_key(\n            api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"entity_selector\"\n        )\n\n        msg = (\n            \"You have access to a knowledge graph that contains \"\n            f\"these entities: {', '.join(self.selected_entities)}. \"\n            \"Your task is to select the relationships that are relevant \"\n            \"to the user's question for subsequent use in a query. Only \"\n            \"return the relationships without their sources or targets, \"\n            \"comma-separated, and without any additional text. Here are the \"\n            \"possible relationships and their source and target entities: \"\n            f\"{rels}.\"\n        )\n\n        conversation.append_system_message(msg)\n\n        res, token_usage, correction = conversation.query(self.question)\n\n        result = res.split(\",\") if msg else []\n\n        if result:\n            for relationship in result:\n                relationship = relationship.strip()\n                if relationship in self.relationships:\n                    self.selected_relationships.append(relationship)\n                    rel_dict = self.relationships[relationship]\n                    label = rel_dict.get(\"label_as_edge\", relationship)\n                    if \"source\" in rel_dict and \"target\" in rel_dict:\n                        self.selected_relationship_labels[label] = {\n                            \"source\": rel_dict[\"source\"],\n                            \"target\": rel_dict[\"target\"],\n                        }\n                    else:\n                        self.selected_relationship_labels[label] = {\n                            \"source\": None,\n                            \"target\": None,\n                        }\n\n        # if we selected relationships that have either source or target which\n        # is not in the selected entities, we add those entities to the selected\n        # entities.\n\n        if self.selected_relationship_labels:\n            for key, value in self.selected_relationship_labels.items():\n                sources = ensure_iterable(value[\"source\"])\n                targets = ensure_iterable(value[\"target\"])\n                for source in sources:\n                    if source not in self.selected_entities:\n                        self.selected_entities.append(source)\n                for target in targets:\n                    if target not in self.selected_entities:\n                        self.selected_entities.append(target)\n\n        return bool(result)\n\n    def _select_properties(\n        self,\n    ):\n        \"\"\"\n\n        Given a question (optionally provided, but in the standard use case\n        reused from the entity selection step) and the selected entities, select\n        the properties that are relevant to the question and store them in\n        the dictionary `selected_properties`.\n\n        Returns:\n            True if at least one property was selected, False otherwise.\n\n        \"\"\"\n\n        if not self.question:\n            raise ValueError(\n                \"No question found. Please make sure to run entity and \"\n                \"relationship selection first.\"\n            )\n\n        if not self.selected_entities and not self.selected_relationships:\n            raise ValueError(\n                \"No entities or relationships provided, and none available \"\n                \"from entity selection step. Please provide \"\n                \"entities/relationships or run the entity selection \"\n                \"(`select_entities()`) step first.\"\n            )\n\n        e_props = {}\n        for entity in self.selected_entities:\n            if self.entities[entity].get(\"properties\"):\n                e_props[entity] = list(\n                    self.entities[entity][\"properties\"].keys()\n                )\n\n        r_props = {}\n        for relationship in self.selected_relationships:\n            if self.relationships[relationship].get(\"properties\"):\n                r_props[relationship] = list(\n                    self.relationships[relationship][\"properties\"].keys()\n                )\n\n        msg = (\n            \"You have access to a knowledge graph that contains entities and \"\n            \"relationships. They have the following properties. Entities:\"\n            f\"{e_props}, Relationships: {r_props}. \"\n            \"Your task is to select the properties that are relevant to the \"\n            \"user's question for subsequent use in a query. Only return the \"\n            \"entities and relationships with their relevant properties in JSON \"\n            \"format, without any additional text. Return the \"\n            \"entities/relationships as top-level dictionary keys, and their \"\n            \"properties as dictionary values. \"\n            \"Do not return properties that are not relevant to the question.\"\n        )\n\n        conversation = GptConversation(\n            model_name=self.model_name,\n            prompts={},\n            correct=False,\n        )\n\n        conversation.set_api_key(\n            api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"property_selector\"\n        )\n\n        conversation.append_system_message(msg)\n\n        msg, token_usage, correction = conversation.query(self.question)\n\n        self.selected_properties = json.loads(msg) if msg else {}\n\n        return bool(self.selected_properties)\n\n    def _generate_query(\n        self,\n        question: str,\n        entities: list,\n        relationships: dict,\n        properties: dict,\n        query_language: str,\n    ):\n        \"\"\"\n        Generate a query in the specified query language that answers the user's\n        question.\n\n        Args:\n            question: A user's question.\n\n            entities: A list of entities that are relevant to the question.\n\n            relationships: A list of relationships that are relevant to the\n                question.\n\n            properties: A dictionary of properties that are relevant to the\n                question.\n\n            query_language: The language of the query to generate.\n        \"\"\"\n        msg = (\n            f\"Generate a database query in {query_language} that answers \"\n            f\"the user's question. \"\n            f\"You can use the following entities: {entities}, \"\n            f\"relationships: {list(relationships.keys())}, and \"\n            f\"properties: {properties}. \"\n        )\n\n        for relationship, values in relationships.items():\n            self._expand_pairs(relationship, values)\n\n        if self.rel_directions:\n            msg += \"Given the following valid combinations of source, relationship, and target: \"\n            for key, value in self.rel_directions.items():\n                for pair in value:\n                    msg += f\"'(:{pair[0]})-(:{key})-&gt;(:{pair[1]})', \"\n            msg += f\"generate a {query_language} query using one of these combinations. \"\n\n        msg += \"Only return the query, without any additional text.\"\n\n        conversation = GptConversation(\n            model_name=self.model_name,\n            prompts={},\n            correct=False,\n        )\n\n        conversation.set_api_key(\n            api_key=os.getenv(\"OPENAI_API_KEY\"), user=\"query_generator\"\n        )\n\n        conversation.append_system_message(msg)\n\n        out_msg, token_usage, correction = conversation.query(question)\n\n        return out_msg\n\n    def _expand_pairs(self, relationship, values):\n        if not self.rel_directions.get(relationship):\n            self.rel_directions[relationship] = []\n        if isinstance(values[\"source\"], list):\n            for source in values[\"source\"]:\n                if isinstance(values[\"target\"], list):\n                    for target in values[\"target\"]:\n                        self.rel_directions[relationship].append(\n                            (source, target)\n                        )\n                else:\n                    self.rel_directions[relationship].append(\n                        (source, values[\"target\"])\n                    )\n        elif isinstance(values[\"target\"], list):\n            for target in values[\"target\"]:\n                self.rel_directions[relationship].append(\n                    (values[\"source\"], target)\n                )\n        else:\n            self.rel_directions[relationship].append(\n                (values[\"source\"], values[\"target\"])\n            )\n</code></pre>"},{"location":"prompts-reference/#biochatter.prompts.BioCypherPromptEngine.__init__","title":"<code>__init__(schema_config_or_info_path=None, schema_config_or_info_dict=None, model_name='gpt-3.5-turbo')</code>","text":"<p>Given a biocypher schema configuration, extract the entities and relationships, and for each extract their mode of representation (node or edge), properties, and identifier namespace. Using these data, allow the generation of prompts for a large language model, informing it of the schema constituents and their properties, to enable the parameterisation of function calls to a knowledge graph.</p> <p>Parameters:</p> Name Type Description Default <code>schema_config_or_info_path</code> <code>Optional[str]</code> <p>Path to a biocypher schema configuration file or the extended schema information output generated by BioCypher's <code>write_schema_info</code> function (preferred).</p> <code>None</code> <code>schema_config_or_info_dict</code> <code>Optional[dict]</code> <p>A dictionary containing the schema configuration file or the extended schema information output generated by BioCypher's <code>write_schema_info</code> function (preferred).</p> <code>None</code> Todo <p>inject conversation directly instead of specifying model name?</p> Source code in <code>biochatter/prompts.py</code> <pre><code>def __init__(\n    self,\n    schema_config_or_info_path: Optional[str] = None,\n    schema_config_or_info_dict: Optional[dict] = None,\n    model_name: str = \"gpt-3.5-turbo\",\n):\n    \"\"\"\n\n    Given a biocypher schema configuration, extract the entities and\n    relationships, and for each extract their mode of representation (node\n    or edge), properties, and identifier namespace. Using these data, allow\n    the generation of prompts for a large language model, informing it of\n    the schema constituents and their properties, to enable the\n    parameterisation of function calls to a knowledge graph.\n\n    Args:\n        schema_config_or_info_path: Path to a biocypher schema configuration\n            file or the extended schema information output generated by\n            BioCypher's `write_schema_info` function (preferred).\n\n        schema_config_or_info_dict: A dictionary containing the schema\n            configuration file or the extended schema information output\n            generated by BioCypher's `write_schema_info` function\n            (preferred).\n\n    Todo:\n        inject conversation directly instead of specifying model name?\n    \"\"\"\n\n    if not schema_config_or_info_path and not schema_config_or_info_dict:\n        raise ValueError(\n            \"Please provide the schema configuration or schema info as a \"\n            \"path to a file or as a dictionary.\"\n        )\n\n    if schema_config_or_info_path and schema_config_or_info_dict:\n        raise ValueError(\n            \"Please provide the schema configuration or schema info as a \"\n            \"path to a file or as a dictionary, not both.\"\n        )\n\n    if schema_config_or_info_path:\n        # read the schema configuration\n        with open(schema_config_or_info_path, \"r\") as f:\n            schema_config = yaml.safe_load(f)\n    elif schema_config_or_info_dict:\n        schema_config = schema_config_or_info_dict\n\n    # check whether it is the original schema config or the output of\n    # biocypher info\n    is_schema_info = schema_config.get(\"is_schema_info\", False)\n\n    # extract the entities and relationships: each top level key that has\n    # a 'represented_as' key\n    self.entities = {}\n    self.relationships = {}\n    if not is_schema_info:\n        for key, value in schema_config.items():\n            # hacky, better with biocypher output\n            name_indicates_relationship = (\n                \"interaction\" in key.lower() or \"association\" in key.lower()\n            )\n            if \"represented_as\" in value:\n                if (\n                    value[\"represented_as\"] == \"node\"\n                    and not name_indicates_relationship\n                ):\n                    self.entities[sentencecase_to_pascalcase(key)] = value\n                elif (\n                    value[\"represented_as\"] == \"node\"\n                    and name_indicates_relationship\n                ):\n                    self.relationships[\n                        sentencecase_to_pascalcase(key)\n                    ] = value\n                elif value[\"represented_as\"] == \"edge\":\n                    self.relationships[\n                        sentencecase_to_pascalcase(key)\n                    ] = value\n    else:\n        for key, value in schema_config.items():\n            if not isinstance(value, dict):\n                continue\n            if value.get(\"present_in_knowledge_graph\", None) == False:\n                continue\n            if value.get(\"is_relationship\", None) == False:\n                self.entities[sentencecase_to_pascalcase(key)] = value\n            elif value.get(\"is_relationship\", None) == True:\n                value = self._capitalise_source_and_target(value)\n                self.relationships[sentencecase_to_pascalcase(key)] = value\n\n    self.question = \"\"\n    self.selected_entities = []\n    self.selected_relationships = []  # used in property selection\n    self.selected_relationship_labels = {}  # copy to deal with labels that\n    # are not the same as the relationship name, used in query generation\n    # dictionary to also include source and target types\n    self.rel_directions = {}\n    self.model_name = model_name\n</code></pre>"},{"location":"prompts-reference/#biochatter.prompts.BioCypherPromptEngine.generate_query","title":"<code>generate_query(question, query_language)</code>","text":"<p>Wrap entity and property selection and query generation; return the generated query.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>A user's question.</p> required <code>query_language</code> <code>str</code> <p>The language of the query to generate.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A database query that could answer the user's question.</p> Source code in <code>biochatter/prompts.py</code> <pre><code>def generate_query(self, question: str, query_language: str) -&gt; str:\n    \"\"\"\n    Wrap entity and property selection and query generation; return the\n    generated query.\n\n    Args:\n        question: A user's question.\n\n        query_language: The language of the query to generate.\n\n    Returns:\n        A database query that could answer the user's question.\n    \"\"\"\n\n    success1 = self._select_entities(question)\n    if not success1:\n        raise ValueError(\n            \"Entity selection failed. Please try again with a different \"\n            \"question.\"\n        )\n    success2 = self._select_relationships()\n    if not success2:\n        raise ValueError(\n            \"Relationship selection failed. Please try again with a \"\n            \"different question.\"\n        )\n    success3 = self._select_properties()\n    if not success3:\n        raise ValueError(\n            \"Property selection failed. Please try again with a different \"\n            \"question.\"\n        )\n\n    return self._generate_query(\n        question=question,\n        entities=self.selected_entities,\n        relationships=self.selected_relationship_labels,\n        properties=self.selected_properties,\n        query_language=query_language,\n    )\n</code></pre>"},{"location":"rag/","title":"Retrieval Augmented Generation","text":"<p>To connect to a vector database for using semantic similarity search and retrieval augmented generation (RAG), we provide an implementation that connects to a Milvus instance (local or remote).  These functions are provided by the modules <code>vectorstore_host.py</code> (for maintaining the connection) and <code>vectorstore.py</code> (for performing embeddings and search).</p> <p>This is implemented in the ChatGSE Docker workflow and the BioChatter Docker compose found in this repository.  To start Milvus on its own in these repositories, you can call <code>docker compose up -d standalone</code> (<code>standalone</code> being the Milvus endpoint, which starts two other services alongside it).</p>"},{"location":"rag/#connecting","title":"Connecting","text":"<p>To connect to a vector DB host, we can use the corresponding class:</p> <pre><code>from biochatter.vectorstore_host import VectorDatabaseHostMilvus\n\ndbHost = VectorDatabaseHostMilvus(\n    embedding_func=OpenAIEmbeddings(),\n    connection_args={\"host\": _HOST, \"port\": _PORT},\n    embedding_collection_name=EMBEDDING_NAME,\n    metadata_collection_name=METADATA_NAME\n)\n</code></pre> <p>This establishes a connection with the vector database (using a host IP and port) and uses two collections, one for the embeddings and one for the metadata of embedded text (e.g. the title and authors of the paper that was embedded).</p>"},{"location":"rag/#embedding-documents","title":"Embedding documents","text":"<p>To embed text from documents, we use the LangChain and BioChatter functionalities for processing and passing the text to the vector database.</p> <pre><code>from biochatter.vectorstore import DocumentReader()\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# read and split document at `pdf_path`\nreader = DocumentReader()\ndocs = reader.load_document(pdf_path)\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap,\n    separators=[\" \", \",\", \"\\n\"],\n)\nsplit_text = text_splitter.split_documents(docs)\n\n# embed and store embeddings in the connected vector DB\ndoc_id = dbHost.store_embeddings(splitted_docs)\n</code></pre> <p>The dbHost class takes care of calling an embedding model, storing the embedding in the database, and returning a document ID that can be used to refer to the stored document.</p>"},{"location":"rag/#semantic-search","title":"Semantic search","text":"<p>To perform a semantic similarity search, all that is left to do is pass a question or statement to the <code>dbHost</code>, which will be embedded and compared to the present embeddings, returning a number <code>k</code> most similar text fragments.</p> <pre><code>results = dbHost.similarity_search(\n    query=\"Semantic similarity search query\",\n    k=3,\n)\n</code></pre>"},{"location":"rag/#vectorstore-management","title":"Vectorstore management","text":"<p>Using the collections we created at setup, we can delete entries in the vector database using their IDs. We can also return a list of all collected docs to determine which we want to delete.</p> <pre><code>docs = dbHost.get_all_documents()\nres = dbHost.remove_document(docs[0][\"id\"])\n</code></pre>"},{"location":"vectorstore-reference/","title":"Vectorstore module","text":"<p>Here we handle the application of vectorstore services to retrieval augmented generation tasks.</p>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentEmbedder","title":"<code>DocumentEmbedder</code>","text":"Source code in <code>biochatter/vectorstore.py</code> <pre><code>class DocumentEmbedder:\n    def __init__(\n        self,\n        use_prompt: bool = True,\n        used: bool = False,\n        online: bool = False,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 0,\n        split_by_characters: bool = True,\n        separators: Optional[list] = None,\n        n_results: int = 3,\n        model: Optional[str] = \"text-embedding-ada-002\",\n        vector_db_vendor: Optional[str] = None,\n        connection_args: Optional[dict] = None,\n        embedding_collection_name: Optional[str] = None,\n        metadata_collection_name: Optional[str] = None,\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n        embeddings: Optional[OpenAIEmbeddings | XinferenceEmbeddings] = None,\n    ) -&gt; None:\n        \"\"\"\n        Class that handles the retrieval augmented generation (RAG) functionality\n        of BioChatter. It splits text into chunks, embeds them, and stores them in\n        a vector database. It can then be used to do similarity search on the\n        database.\n\n        Args:\n\n            use_prompt (bool, optional): whether to use RAG (ChatGSE setting).\n            Defaults to True.\n\n            used (bool, optional): whether RAG has been used (ChatGSE setting).\n            Defaults to False.\n\n            online (bool, optional): whether we are running ChatGSE online.\n            Defaults to False.\n\n            chunk_size (int, optional): size of chunks to split text into.\n            Defaults to 1000.\n\n            chunk_overlap (int, optional): overlap between chunks. Defaults to 0.\n\n            split_by_characters (bool, optional): whether to split by characters\n            or tokens. Defaults to True.\n\n            separators (Optional[list], optional): list of separators to use when\n            splitting by characters. Defaults to [\" \", \",\", \"\\n\"].\n\n            n_results (int, optional): number of results to return from\n            similarity search. Defaults to 3.\n\n            model (Optional[str], optional): name of model to use for embeddings.\n            Defaults to 'text-embedding-ada-002'.\n\n            vector_db_vendor (Optional[str], optional): name of vector database\n            to use. Defaults to Milvus.\n\n            connection_args (Optional[dict], optional): arguments to pass to\n            vector database connection. Defaults to None.\n\n            embedding_collection_name (Optional[str], optional): name of\n            collection to store embeddings in. Defaults to 'DocumentEmbeddings'.\n\n            metadata_collection_name (Optional[str], optional): name of\n            collection to store metadata in. Defaults to 'DocumentMetadata'.\n\n            api_key (Optional[str], optional): OpenAI API key. Defaults to None.\n\n            base_url (Optional[str], optional): base url of OpenAI API.\n\n            embeddings (Optional[OpenAIEmbeddings | XinferenceEmbeddings],\n            optional): Embeddings object to use. Defaults to OpenAI.\n\n        \"\"\"\n        self.use_prompt = use_prompt\n        self.used = used\n        self.online = online\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.separators = separators or [\" \", \",\", \"\\n\"]\n        self.n_results = n_results\n        self.split_by_characters = split_by_characters\n        self.model_name = model\n\n        # TODO API Key handling to central config?\n        if base_url:\n            openai.api_base = base_url\n\n        if embeddings:\n            self.embeddings = embeddings\n        else:\n            if not self.online:\n                self.embeddings = OpenAIEmbeddings(\n                    openai_api_key=api_key, model=model\n                )\n            else:\n                self.embeddings = None\n\n        # connection arguments\n        self.connection_args = connection_args or {\n            \"host\": \"127.0.0.1\",\n            \"port\": \"19530\",\n        }\n        self.embedding_collection_name = embedding_collection_name\n        self.metadata_collection_name = metadata_collection_name\n\n        # TODO: vector db selection\n        self.vector_db_vendor = vector_db_vendor or \"milvus\"\n        # instantiate VectorDatabaseHost\n        self.database_host = None\n        self._init_database_host()\n\n    def _set_embeddings(self, embeddings):\n        print(\"setting embedder\")\n        self.embeddings = embeddings\n\n    def _init_database_host(self):\n        if self.vector_db_vendor == \"milvus\":\n            self.database_host = VectorDatabaseHostMilvus(\n                embedding_func=self.embeddings,\n                connection_args=self.connection_args,\n                embedding_collection_name=self.embedding_collection_name,\n                metadata_collection_name=self.metadata_collection_name,\n            )\n        else:\n            raise NotImplementedError(self.vector_db_vendor)\n\n    def set_chunk_siue(self, chunk_size: int) -&gt; None:\n        self.chunk_size = chunk_size\n\n    def set_chunk_overlap(self, chunk_overlap: int) -&gt; None:\n        self.chunk_overlap = chunk_overlap\n\n    def set_separators(self, separators: list) -&gt; None:\n        self.separators = separators\n\n    def _characters_splitter(self) -&gt; RecursiveCharacterTextSplitter:\n        return RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            separators=self.separators,\n        )\n\n    def _tokens_splitter(self) -&gt; RecursiveCharacterTextSplitter:\n        DEFAULT_OPENAI_MODEL = \"gpt-3.5-turbo\"\n        HUGGINGFACE_MODELS = [\"bigscience/bloom\"]\n        if self.model_name and self.model_name in HUGGINGFACE_MODELS:\n            tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n            return RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n                tokenizer,\n                chunk_size=self.chunk_size,\n                chunk_overlap=self.chunk_overlap,\n                separators=self.separators,\n            )\n        else:\n            return RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n                encoding_name=\"\",\n                model_name=DEFAULT_OPENAI_MODEL\n                if not self.model_name\n                else self.model_name,\n                chunk_size=self.chunk_size,\n                chunk_overlap=self.chunk_overlap,\n                separators=self.separators,\n            )\n\n    def _text_splitter(self) -&gt; RecursiveCharacterTextSplitter:\n        return (\n            self._characters_splitter()\n            if self.split_by_characters\n            else self._tokens_splitter()\n        )\n\n    def save_document(self, doc: List[Document]) -&gt; str:\n        \"\"\"\n        This function saves document to the vector database\n        Args:\n            doc List[Document]: document content, read with DocumentReader load_document(),\n                or document_from_pdf(), document_from_txt()\n        Returns:\n            str: document id, which can be used to remove an uploaded document with remove_document()\n        \"\"\"\n        splitted = self._split_document(doc)\n        return self._store_embeddings(splitted)\n\n    def _split_document(self, document: List[Document]) -&gt; List[Document]:\n        text_splitter = self._text_splitter()\n        return text_splitter.split_documents(document)\n\n    def _store_embeddings(self, doc: List[Document]) -&gt; str:\n        return self.database_host.store_embeddings(documents=doc)\n\n    def similarity_search(self, query: str, k: int = 3):\n        \"\"\"\n        Returns top n closest matches to query from vector store.\n\n        Args:\n            query (str): query string\n\n            k (int, optional): number of closest matches to return. Defaults to\n            3.\n\n        \"\"\"\n        return self.database_host.similarity_search(\n            query=query, k=k or self.n_results\n        )\n\n    def connect(self) -&gt; None:\n        self.database_host.connect()\n\n    def get_all_documents(self) -&gt; List[Dict]:\n        return self.database_host.get_all_documents()\n\n    def remove_document(self, doc_id: str) -&gt; None:\n        self.database_host.remove_document(doc_id)\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentEmbedder.__init__","title":"<code>__init__(use_prompt=True, used=False, online=False, chunk_size=1000, chunk_overlap=0, split_by_characters=True, separators=None, n_results=3, model='text-embedding-ada-002', vector_db_vendor=None, connection_args=None, embedding_collection_name=None, metadata_collection_name=None, api_key=None, base_url=None, embeddings=None)</code>","text":"<pre><code>    Class that handles the retrieval augmented generation (RAG) functionality\n    of BioChatter. It splits text into chunks, embeds them, and stores them in\n    a vector database. It can then be used to do similarity search on the\n    database.\n\n    Args:\n\n        use_prompt (bool, optional): whether to use RAG (ChatGSE setting).\n        Defaults to True.\n\n        used (bool, optional): whether RAG has been used (ChatGSE setting).\n        Defaults to False.\n\n        online (bool, optional): whether we are running ChatGSE online.\n        Defaults to False.\n\n        chunk_size (int, optional): size of chunks to split text into.\n        Defaults to 1000.\n\n        chunk_overlap (int, optional): overlap between chunks. Defaults to 0.\n\n        split_by_characters (bool, optional): whether to split by characters\n        or tokens. Defaults to True.\n\n        separators (Optional[list], optional): list of separators to use when\n        splitting by characters. Defaults to [\" \", \",\", \"\n</code></pre> <p>\"].</p> <pre><code>        n_results (int, optional): number of results to return from\n        similarity search. Defaults to 3.\n\n        model (Optional[str], optional): name of model to use for embeddings.\n        Defaults to 'text-embedding-ada-002'.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n        to use. Defaults to Milvus.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n        vector database connection. Defaults to None.\n\n        embedding_collection_name (Optional[str], optional): name of\n        collection to store embeddings in. Defaults to 'DocumentEmbeddings'.\n\n        metadata_collection_name (Optional[str], optional): name of\n        collection to store metadata in. Defaults to 'DocumentMetadata'.\n\n        api_key (Optional[str], optional): OpenAI API key. Defaults to None.\n\n        base_url (Optional[str], optional): base url of OpenAI API.\n\n        embeddings (Optional[OpenAIEmbeddings | XinferenceEmbeddings],\n        optional): Embeddings object to use. Defaults to OpenAI.\n</code></pre> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def __init__(\n    self,\n    use_prompt: bool = True,\n    used: bool = False,\n    online: bool = False,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 0,\n    split_by_characters: bool = True,\n    separators: Optional[list] = None,\n    n_results: int = 3,\n    model: Optional[str] = \"text-embedding-ada-002\",\n    vector_db_vendor: Optional[str] = None,\n    connection_args: Optional[dict] = None,\n    embedding_collection_name: Optional[str] = None,\n    metadata_collection_name: Optional[str] = None,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    embeddings: Optional[OpenAIEmbeddings | XinferenceEmbeddings] = None,\n) -&gt; None:\n    \"\"\"\n    Class that handles the retrieval augmented generation (RAG) functionality\n    of BioChatter. It splits text into chunks, embeds them, and stores them in\n    a vector database. It can then be used to do similarity search on the\n    database.\n\n    Args:\n\n        use_prompt (bool, optional): whether to use RAG (ChatGSE setting).\n        Defaults to True.\n\n        used (bool, optional): whether RAG has been used (ChatGSE setting).\n        Defaults to False.\n\n        online (bool, optional): whether we are running ChatGSE online.\n        Defaults to False.\n\n        chunk_size (int, optional): size of chunks to split text into.\n        Defaults to 1000.\n\n        chunk_overlap (int, optional): overlap between chunks. Defaults to 0.\n\n        split_by_characters (bool, optional): whether to split by characters\n        or tokens. Defaults to True.\n\n        separators (Optional[list], optional): list of separators to use when\n        splitting by characters. Defaults to [\" \", \",\", \"\\n\"].\n\n        n_results (int, optional): number of results to return from\n        similarity search. Defaults to 3.\n\n        model (Optional[str], optional): name of model to use for embeddings.\n        Defaults to 'text-embedding-ada-002'.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n        to use. Defaults to Milvus.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n        vector database connection. Defaults to None.\n\n        embedding_collection_name (Optional[str], optional): name of\n        collection to store embeddings in. Defaults to 'DocumentEmbeddings'.\n\n        metadata_collection_name (Optional[str], optional): name of\n        collection to store metadata in. Defaults to 'DocumentMetadata'.\n\n        api_key (Optional[str], optional): OpenAI API key. Defaults to None.\n\n        base_url (Optional[str], optional): base url of OpenAI API.\n\n        embeddings (Optional[OpenAIEmbeddings | XinferenceEmbeddings],\n        optional): Embeddings object to use. Defaults to OpenAI.\n\n    \"\"\"\n    self.use_prompt = use_prompt\n    self.used = used\n    self.online = online\n    self.chunk_size = chunk_size\n    self.chunk_overlap = chunk_overlap\n    self.separators = separators or [\" \", \",\", \"\\n\"]\n    self.n_results = n_results\n    self.split_by_characters = split_by_characters\n    self.model_name = model\n\n    # TODO API Key handling to central config?\n    if base_url:\n        openai.api_base = base_url\n\n    if embeddings:\n        self.embeddings = embeddings\n    else:\n        if not self.online:\n            self.embeddings = OpenAIEmbeddings(\n                openai_api_key=api_key, model=model\n            )\n        else:\n            self.embeddings = None\n\n    # connection arguments\n    self.connection_args = connection_args or {\n        \"host\": \"127.0.0.1\",\n        \"port\": \"19530\",\n    }\n    self.embedding_collection_name = embedding_collection_name\n    self.metadata_collection_name = metadata_collection_name\n\n    # TODO: vector db selection\n    self.vector_db_vendor = vector_db_vendor or \"milvus\"\n    # instantiate VectorDatabaseHost\n    self.database_host = None\n    self._init_database_host()\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentEmbedder.save_document","title":"<code>save_document(doc)</code>","text":"<p>This function saves document to the vector database Args:     doc List[Document]: document content, read with DocumentReader load_document(),         or document_from_pdf(), document_from_txt() Returns:     str: document id, which can be used to remove an uploaded document with remove_document()</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def save_document(self, doc: List[Document]) -&gt; str:\n    \"\"\"\n    This function saves document to the vector database\n    Args:\n        doc List[Document]: document content, read with DocumentReader load_document(),\n            or document_from_pdf(), document_from_txt()\n    Returns:\n        str: document id, which can be used to remove an uploaded document with remove_document()\n    \"\"\"\n    splitted = self._split_document(doc)\n    return self._store_embeddings(splitted)\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentEmbedder.similarity_search","title":"<code>similarity_search(query, k=3)</code>","text":"<p>Returns top n closest matches to query from vector store.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query string</p> required <code>k</code> <code>int</code> <p>number of closest matches to return. Defaults to</p> <code>3</code> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def similarity_search(self, query: str, k: int = 3):\n    \"\"\"\n    Returns top n closest matches to query from vector store.\n\n    Args:\n        query (str): query string\n\n        k (int, optional): number of closest matches to return. Defaults to\n        3.\n\n    \"\"\"\n    return self.database_host.similarity_search(\n        query=query, k=k or self.n_results\n    )\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentReader","title":"<code>DocumentReader</code>","text":"Source code in <code>biochatter/vectorstore.py</code> <pre><code>class DocumentReader:\n    def load_document(self, path: str) -&gt; List[Document]:\n        \"\"\"\n        Loads a document from a path; accepts txt and pdf files. Txt files are\n        loaded as-is, pdf files are converted to text using fitz.\n\n        Args:\n            path (str): path to document\n\n        Returns:\n            List[Document]: list of documents\n        \"\"\"\n        if path.endswith(\".txt\"):\n            loader = TextLoader(path)\n            return loader.load()\n\n        elif path.endswith(\".pdf\"):\n            doc = fitz.open(path)\n            text = \"\"\n            for page in doc:\n                text += page.get_text()\n\n            meta = {k: v for k, v in doc.metadata.items() if v}\n            meta.update({\"source\": path})\n\n            return [\n                Document(\n                    page_content=text,\n                    metadata=meta,\n                )\n            ]\n\n    def document_from_pdf(self, pdf: bytes) -&gt; List[Document]:\n        \"\"\"\n        Receive a byte representation of a pdf file and return a list of Documents\n        with metadata.\n\n        Args:\n            pdf (bytes): byte representation of pdf file\n\n        Returns:\n            List[Document]: list of documents\n        \"\"\"\n        doc = fitz.open(stream=pdf, filetype=\"pdf\")\n        text = \"\"\n        for page in doc:\n            text += page.get_text()\n\n        meta = {k: v for k, v in doc.metadata.items() if v}\n        meta.update({\"source\": \"pdf\"})\n\n        return [\n            Document(\n                page_content=text,\n                metadata=meta,\n            )\n        ]\n\n    def document_from_txt(self, txt: bytes) -&gt; List[Document]:\n        \"\"\"\n        Receive a byte representation of a txt file and return a list of Documents\n        with metadata.\n\n        Args:\n            txt (bytes): byte representation of txt file\n\n        Returns:\n            List[Document]: list of documents\n        \"\"\"\n        meta = {\"source\": \"txt\"}\n        return [\n            Document(\n                page_content=txt,\n                metadata=meta,\n            )\n        ]\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentReader.document_from_pdf","title":"<code>document_from_pdf(pdf)</code>","text":"<p>Receive a byte representation of a pdf file and return a list of Documents with metadata.</p> <p>Parameters:</p> Name Type Description Default <code>pdf</code> <code>bytes</code> <p>byte representation of pdf file</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of documents</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def document_from_pdf(self, pdf: bytes) -&gt; List[Document]:\n    \"\"\"\n    Receive a byte representation of a pdf file and return a list of Documents\n    with metadata.\n\n    Args:\n        pdf (bytes): byte representation of pdf file\n\n    Returns:\n        List[Document]: list of documents\n    \"\"\"\n    doc = fitz.open(stream=pdf, filetype=\"pdf\")\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n\n    meta = {k: v for k, v in doc.metadata.items() if v}\n    meta.update({\"source\": \"pdf\"})\n\n    return [\n        Document(\n            page_content=text,\n            metadata=meta,\n        )\n    ]\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentReader.document_from_txt","title":"<code>document_from_txt(txt)</code>","text":"<p>Receive a byte representation of a txt file and return a list of Documents with metadata.</p> <p>Parameters:</p> Name Type Description Default <code>txt</code> <code>bytes</code> <p>byte representation of txt file</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of documents</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def document_from_txt(self, txt: bytes) -&gt; List[Document]:\n    \"\"\"\n    Receive a byte representation of a txt file and return a list of Documents\n    with metadata.\n\n    Args:\n        txt (bytes): byte representation of txt file\n\n    Returns:\n        List[Document]: list of documents\n    \"\"\"\n    meta = {\"source\": \"txt\"}\n    return [\n        Document(\n            page_content=txt,\n            metadata=meta,\n        )\n    ]\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.DocumentReader.load_document","title":"<code>load_document(path)</code>","text":"<p>Loads a document from a path; accepts txt and pdf files. Txt files are loaded as-is, pdf files are converted to text using fitz.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to document</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of documents</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def load_document(self, path: str) -&gt; List[Document]:\n    \"\"\"\n    Loads a document from a path; accepts txt and pdf files. Txt files are\n    loaded as-is, pdf files are converted to text using fitz.\n\n    Args:\n        path (str): path to document\n\n    Returns:\n        List[Document]: list of documents\n    \"\"\"\n    if path.endswith(\".txt\"):\n        loader = TextLoader(path)\n        return loader.load()\n\n    elif path.endswith(\".pdf\"):\n        doc = fitz.open(path)\n        text = \"\"\n        for page in doc:\n            text += page.get_text()\n\n        meta = {k: v for k, v in doc.metadata.items() if v}\n        meta.update({\"source\": path})\n\n        return [\n            Document(\n                page_content=text,\n                metadata=meta,\n            )\n        ]\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.XinferenceDocumentEmbedder","title":"<code>XinferenceDocumentEmbedder</code>","text":"<p>             Bases: <code>DocumentEmbedder</code></p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>class XinferenceDocumentEmbedder(DocumentEmbedder):\n    def __init__(\n        self,\n        use_prompt: bool = True,\n        used: bool = False,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 0,\n        split_by_characters: bool = True,\n        separators: Optional[list] = None,\n        n_results: int = 3,\n        model: Optional[str] = \"auto\",\n        vector_db_vendor: Optional[str] = None,\n        connection_args: Optional[dict] = None,\n        embedding_collection_name: Optional[str] = None,\n        metadata_collection_name: Optional[str] = None,\n        api_key: Optional[str] = \"none\",\n        base_url: Optional[str] = None,\n    ):\n        \"\"\"\n        Extension of the DocumentEmbedder class that uses Xinference for\n        embeddings.\n\n        Args:\n\n            use_prompt (bool, optional): whether to use RAG (ChatGSE setting).\n\n            used (bool, optional): whether RAG has been used (ChatGSE setting).\n\n            chunk_size (int, optional): size of chunks to split text into.\n\n            chunk_overlap (int, optional): overlap between chunks.\n\n            split_by_characters (bool, optional): whether to split by characters\n            or tokens.\n\n            separators (Optional[list], optional): list of separators to use when\n            splitting by characters.\n\n            n_results (int, optional): number of results to return from\n            similarity search.\n\n            model (Optional[str], optional): name of model to use for embeddings.\n            Can be \"auto\" to use the first available model.\n\n            vector_db_vendor (Optional[str], optional): name of vector database\n            to use.\n\n            connection_args (Optional[dict], optional): arguments to pass to\n            vector database connection.\n\n            embedding_collection_name (Optional[str], optional): name of\n            collection to store embeddings in.\n\n            metadata_collection_name (Optional[str], optional): name of\n            collection to store metadata in.\n\n            api_key (Optional[str], optional): Xinference API key.\n\n            base_url (Optional[str], optional): base url of Xinference API.\n\n        \"\"\"\n        self.model_name = model\n        self.client = Client(base_url=base_url)\n        self.models = {}\n        self.load_models()\n\n        if self.model_name is None or self.model_name == \"auto\":\n            self.model_name = self.list_models_by_type(\"embedding\")[0]\n        self.model_uid = self.models[self.model_name][\"id\"]\n\n        super().__init__(\n            use_prompt=use_prompt,\n            used=used,\n            online=True,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            split_by_characters=split_by_characters,\n            separators=separators,\n            n_results=n_results,\n            model=model,\n            vector_db_vendor=vector_db_vendor,\n            connection_args=connection_args,\n            embedding_collection_name=embedding_collection_name,\n            metadata_collection_name=metadata_collection_name,\n            api_key=api_key,\n            base_url=base_url,\n            embeddings=XinferenceEmbeddings(\n                server_url=base_url, model_uid=self.model_uid\n            ),\n        )\n\n    def load_models(self):\n        \"\"\"\n        Return all models that are currently available on the Xinference server.\n\n        Returns:\n            dict: dict of models\n        \"\"\"\n        for id, model in self.client.list_models().items():\n            model[\"id\"] = id\n            self.models[model[\"model_name\"]] = model\n\n    def list_models_by_type(self, type: str):\n        \"\"\"\n        Return all models of a certain type that are currently available on the\n        Xinference server.\n\n        Args:\n            type (str): type of model to list (e.g. \"embedding\", \"chat\")\n\n        Returns:\n            List[str]: list of model names\n        \"\"\"\n        names = []\n        for name, model in self.models.items():\n            if \"model_ability\" in model:\n                if type in model[\"model_ability\"]:\n                    names.append(name)\n            elif model[\"model_type\"] == type:\n                names.append(name)\n        return names\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.XinferenceDocumentEmbedder.__init__","title":"<code>__init__(use_prompt=True, used=False, chunk_size=1000, chunk_overlap=0, split_by_characters=True, separators=None, n_results=3, model='auto', vector_db_vendor=None, connection_args=None, embedding_collection_name=None, metadata_collection_name=None, api_key='none', base_url=None)</code>","text":"<p>Extension of the DocumentEmbedder class that uses Xinference for embeddings.</p> <p>Args:</p> <pre><code>use_prompt (bool, optional): whether to use RAG (ChatGSE setting).\n\nused (bool, optional): whether RAG has been used (ChatGSE setting).\n\nchunk_size (int, optional): size of chunks to split text into.\n\nchunk_overlap (int, optional): overlap between chunks.\n\nsplit_by_characters (bool, optional): whether to split by characters\nor tokens.\n\nseparators (Optional[list], optional): list of separators to use when\nsplitting by characters.\n\nn_results (int, optional): number of results to return from\nsimilarity search.\n\nmodel (Optional[str], optional): name of model to use for embeddings.\nCan be \"auto\" to use the first available model.\n\nvector_db_vendor (Optional[str], optional): name of vector database\nto use.\n\nconnection_args (Optional[dict], optional): arguments to pass to\nvector database connection.\n\nembedding_collection_name (Optional[str], optional): name of\ncollection to store embeddings in.\n\nmetadata_collection_name (Optional[str], optional): name of\ncollection to store metadata in.\n\napi_key (Optional[str], optional): Xinference API key.\n\nbase_url (Optional[str], optional): base url of Xinference API.\n</code></pre> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def __init__(\n    self,\n    use_prompt: bool = True,\n    used: bool = False,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 0,\n    split_by_characters: bool = True,\n    separators: Optional[list] = None,\n    n_results: int = 3,\n    model: Optional[str] = \"auto\",\n    vector_db_vendor: Optional[str] = None,\n    connection_args: Optional[dict] = None,\n    embedding_collection_name: Optional[str] = None,\n    metadata_collection_name: Optional[str] = None,\n    api_key: Optional[str] = \"none\",\n    base_url: Optional[str] = None,\n):\n    \"\"\"\n    Extension of the DocumentEmbedder class that uses Xinference for\n    embeddings.\n\n    Args:\n\n        use_prompt (bool, optional): whether to use RAG (ChatGSE setting).\n\n        used (bool, optional): whether RAG has been used (ChatGSE setting).\n\n        chunk_size (int, optional): size of chunks to split text into.\n\n        chunk_overlap (int, optional): overlap between chunks.\n\n        split_by_characters (bool, optional): whether to split by characters\n        or tokens.\n\n        separators (Optional[list], optional): list of separators to use when\n        splitting by characters.\n\n        n_results (int, optional): number of results to return from\n        similarity search.\n\n        model (Optional[str], optional): name of model to use for embeddings.\n        Can be \"auto\" to use the first available model.\n\n        vector_db_vendor (Optional[str], optional): name of vector database\n        to use.\n\n        connection_args (Optional[dict], optional): arguments to pass to\n        vector database connection.\n\n        embedding_collection_name (Optional[str], optional): name of\n        collection to store embeddings in.\n\n        metadata_collection_name (Optional[str], optional): name of\n        collection to store metadata in.\n\n        api_key (Optional[str], optional): Xinference API key.\n\n        base_url (Optional[str], optional): base url of Xinference API.\n\n    \"\"\"\n    self.model_name = model\n    self.client = Client(base_url=base_url)\n    self.models = {}\n    self.load_models()\n\n    if self.model_name is None or self.model_name == \"auto\":\n        self.model_name = self.list_models_by_type(\"embedding\")[0]\n    self.model_uid = self.models[self.model_name][\"id\"]\n\n    super().__init__(\n        use_prompt=use_prompt,\n        used=used,\n        online=True,\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        split_by_characters=split_by_characters,\n        separators=separators,\n        n_results=n_results,\n        model=model,\n        vector_db_vendor=vector_db_vendor,\n        connection_args=connection_args,\n        embedding_collection_name=embedding_collection_name,\n        metadata_collection_name=metadata_collection_name,\n        api_key=api_key,\n        base_url=base_url,\n        embeddings=XinferenceEmbeddings(\n            server_url=base_url, model_uid=self.model_uid\n        ),\n    )\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.XinferenceDocumentEmbedder.list_models_by_type","title":"<code>list_models_by_type(type)</code>","text":"<p>Return all models of a certain type that are currently available on the Xinference server.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>type of model to list (e.g. \"embedding\", \"chat\")</p> required <p>Returns:</p> Type Description <p>List[str]: list of model names</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def list_models_by_type(self, type: str):\n    \"\"\"\n    Return all models of a certain type that are currently available on the\n    Xinference server.\n\n    Args:\n        type (str): type of model to list (e.g. \"embedding\", \"chat\")\n\n    Returns:\n        List[str]: list of model names\n    \"\"\"\n    names = []\n    for name, model in self.models.items():\n        if \"model_ability\" in model:\n            if type in model[\"model_ability\"]:\n                names.append(name)\n        elif model[\"model_type\"] == type:\n            names.append(name)\n    return names\n</code></pre>"},{"location":"vectorstore-reference/#biochatter.vectorstore.XinferenceDocumentEmbedder.load_models","title":"<code>load_models()</code>","text":"<p>Return all models that are currently available on the Xinference server.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>dict of models</p> Source code in <code>biochatter/vectorstore.py</code> <pre><code>def load_models(self):\n    \"\"\"\n    Return all models that are currently available on the Xinference server.\n\n    Returns:\n        dict: dict of models\n    \"\"\"\n    for id, model in self.client.list_models().items():\n        model[\"id\"] = id\n        self.models[model[\"model_name\"]] = model\n</code></pre>"},{"location":"vectorstore_host-reference/","title":"Vectorstore host module","text":"<p>Here we handle connections and management of various vectorstore services.</p>"},{"location":"vectorstore_host-reference/#biochatter.vectorstore_host.VectorDatabaseHostMilvus","title":"<code>VectorDatabaseHostMilvus</code>","text":"<p>The VectorDatabaseHostMilvus class manages vector databases in a connected host database. It manages an embedding collection <code>_col_embeddings:langchain.vectorstores.Milvus</code>, which is the main information on the embedded text fragments and the basis for similarity search, and a metadata collection <code>_col_metadata:pymilvus.Collection</code>, which stores the metadata of the embedded text fragments. A typical workflow includes the following operations:</p> <ol> <li>connect to a host using <code>connect()</code></li> <li>get all documents in the active database using <code>get_all_documents()</code></li> <li>save a number of fragments, usually from a specific document, using     <code>store_embeddings()</code></li> <li>do similarity search among all fragments of the currently active database     using <code>similarity_search()</code></li> <li>remove a document from the currently active database using     <code>remove_document()</code></li> </ol> Source code in <code>biochatter/vectorstore_host.py</code> <pre><code>class VectorDatabaseHostMilvus:\n    \"\"\"\n    The VectorDatabaseHostMilvus class manages vector databases in a connected\n    host database. It manages an embedding collection\n    `_col_embeddings:langchain.vectorstores.Milvus`, which is the main\n    information on the embedded text fragments and the basis for similarity\n    search, and a metadata collection `_col_metadata:pymilvus.Collection`, which\n    stores the metadata of the embedded text fragments. A typical workflow\n    includes the following operations:\n\n    1. connect to a host using `connect()`\n    2. get all documents in the active database using `get_all_documents()`\n    3. save a number of fragments, usually from a specific document, using\n        `store_embeddings()`\n    4. do similarity search among all fragments of the currently active database\n        using `similarity_search()`\n    5. remove a document from the currently active database using\n        `remove_document()`\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_func: Optional[OpenAIEmbeddings] = None,\n        connection_args: Optional[Dict] = None,\n        embedding_collection_name: Optional[str] = None,\n        metadata_collection_name: Optional[str] = None,\n    ):\n        self._embedding_func = embedding_func\n        self._col_embeddings: Optional[Milvus] = None\n        self._col_metadata: Optional[Collection] = None\n        self._connection_args = connection_args or {\n            \"host\": \"127.0.0.1\",\n            \"port\": \"19530\",\n        }\n        self._embedding_name = (\n            embedding_collection_name or DOCUMENT_EMBEDDINGS_COLLECTION_NAME\n        )\n        self._metadata_name = (\n            metadata_collection_name or DOCUMENT_METADATA_COLLECTION_NAME\n        )\n\n    def connect(self) -&gt; None:\n        \"\"\"\n        Connect to a host and read two document collections (the default names\n        are `DocumentEmbeddings` and `DocumentMetadata`) in the currently active\n        database (default database name is `default`); if those document\n        collections don't exist, create the two collections.\n        \"\"\"\n        self._connect(\n            self._connection_args[\"host\"], self._connection_args[\"port\"]\n        )\n        self._init_host()\n\n    def _connect(self, host: str, port: str) -&gt; None:\n        self._connection_args = {\"host\": host, \"port\": port}\n        self.alias = self._create_connection_alias(host, port)\n\n    def _init_host(self) -&gt; None:\n        \"\"\"\n        Initialize host. Will read/create document collection inside currently\n        active database.\n        \"\"\"\n        self._create_collections()\n\n    def _create_connection_alias(self, host: str, port: str) -&gt; str:\n        \"\"\"\n        Connect to host and create a connection alias for metadata collection\n        using a random uuid.\n\n        Args:\n            host (str): host ip address\n            port (str): host port\n\n        Returns:\n            str: connection alias\n        \"\"\"\n        alias = uuid.uuid4().hex\n        try:\n            connections.connect(host=host, port=port, alias=alias)\n            logger.debug(f\"Created new connection using: {alias}\")\n            return alias\n        except MilvusException as e:\n            logger.error(f\"Failed to create  new connection using: {alias}\")\n            raise e\n\n    def _create_collections(self) -&gt; None:\n        \"\"\"\n        Create or load the embedding and metadata collections from the currently\n        active database.\n        \"\"\"\n        embedding_exists = utility.has_collection(\n            self._embedding_name, using=self.alias\n        )\n        meta_exists = utility.has_collection(\n            self._metadata_name,\n            using=self.alias,\n        )\n\n        if embedding_exists:\n            self._load_embeddings_collection()\n        else:\n            self._create_embeddings_collection()\n\n        if meta_exists:\n            self._load_metadata_collection()\n        else:\n            self._create_metadata_collection()\n\n        self._create_metadata_collection_index()\n        self._col_metadata.load()\n\n    def _load_embeddings_collection(self) -&gt; None:\n        \"\"\"\n        Load embeddings collection from currently active database.\n        \"\"\"\n        try:\n            self._col_embeddings = Milvus(\n                embedding_function=self._embedding_func,\n                collection_name=self._embedding_name,\n                connection_args=self._connection_args,\n            )\n        except MilvusException as e:\n            logger.error(\n                f\"Failed to load embeddings collection {self._embedding_name}.\"\n            )\n            raise e\n\n    def _create_embeddings_collection(self) -&gt; None:\n        \"\"\"\n        Create embedding collection.\n        All fields: \"meta_id\", \"vector\"\n        \"\"\"\n        try:\n            self._col_embeddings = Milvus(\n                embedding_function=self._embedding_func,\n                collection_name=self._embedding_name,\n                connection_args=self._connection_args,\n            )\n        except MilvusException as e:\n            logger.error(\n                f\"Failed to create embeddings collection {self._embedding_name}\"\n            )\n            raise e\n\n    def _load_metadata_collection(self) -&gt; None:\n        \"\"\"\n        Load metadata collection from currently active database.\n        \"\"\"\n        self._col_metadata = Collection(\n            self._metadata_name,\n            using=self.alias,\n        )\n        self._col_metadata.load()\n\n    def _create_metadata_collection(self) -&gt; None:\n        \"\"\"\n        Create metadata collection.\n\n        All fields: \"id\", \"name\", \"author\", \"title\", \"format\", \"subject\",\n        \"creator\", \"producer\", \"creationDate\", \"modDate\", \"source\", \"embedding\",\n        \"isDeleted\".\n\n        As the vector database requires a vector field, we will create a fake\n        vector \"embedding\". The field \"isDeleted\" is used to specify if the\n        document is deleted.\n        \"\"\"\n        doc_id = FieldSchema(\n            name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True\n        )\n        doc_name = FieldSchema(\n            name=\"name\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_author = FieldSchema(\n            name=\"author\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_title = FieldSchema(\n            name=\"title\", dtype=DataType.VARCHAR, max_length=1000\n        )\n        doc_format = FieldSchema(\n            name=\"format\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_subject = FieldSchema(\n            name=\"subject\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_creator = FieldSchema(\n            name=\"creator\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_producer = FieldSchema(\n            name=\"producer\", dtype=DataType.VARCHAR, max_length=255\n        )\n        doc_creationDate = FieldSchema(\n            name=\"creationDate\", dtype=DataType.VARCHAR, max_length=64\n        )\n        doc_modDate = FieldSchema(\n            name=\"modDate\", dtype=DataType.VARCHAR, max_length=64\n        )\n        doc_source = FieldSchema(\n            name=\"source\", dtype=DataType.VARCHAR, max_length=1000\n        )\n        embedding = FieldSchema(\n            name=\"embedding\",\n            dtype=DataType.FLOAT_VECTOR,\n            dim=METADATA_VECTOR_DIM,\n        )\n        isDeleted = FieldSchema(\n            name=\"isDeleted\",\n            dtype=DataType.BOOL,\n        )\n        fields = [\n            doc_id,\n            doc_name,\n            doc_author,\n            doc_title,\n            doc_format,\n            doc_subject,\n            doc_creator,\n            doc_producer,\n            doc_creationDate,\n            doc_modDate,\n            doc_source,\n            embedding,\n            isDeleted,\n        ]\n        schema = CollectionSchema(fields=fields)\n        try:\n            self._col_metadata = Collection(\n                name=self._metadata_name, schema=schema, using=self.alias\n            )\n        except MilvusException as e:\n            logger.error(f\"Failed to create collection {self._metadata_name}\")\n            raise e\n\n    def _create_metadata_collection_index(self) -&gt; None:\n        \"\"\"\n        Create index for metadata collection in currently active database.\n        \"\"\"\n        if (\n            not isinstance(self._col_metadata, Collection)\n            or len(self._col_metadata.indexes) &gt; 0\n        ):\n            return\n\n        index_params = {\n            \"metric_type\": \"L2\",\n            \"index_type\": \"HNSW\",\n            \"params\": {\"M\": 8, \"efConstruction\": 64},\n        }\n\n        try:\n            self._col_metadata.create_index(\n                field_name=\"embedding\",\n                index_params=index_params,\n                using=self.alias,\n            )\n        except MilvusException as e:\n            logger.error(\n                \"Failed to create index for meta collection \"\n                f\"{self._metadata_name}.\"\n            )\n            raise e\n\n    def _insert_data(self, documents: List[Document]) -&gt; str:\n        \"\"\"\n        Insert documents into the currently active database.\n\n        Args:\n            documents (List[Documents]): documents array, usually from\n                DocumentReader.load_document, DocumentReader.document_from_pdf,\n                DocumentReader.document_from_txt\n\n        Returns:\n            str: document id\n        \"\"\"\n        if len(documents) == 0:\n            return None\n        metadata = [documents[0].metadata]\n        aligned_metadata = align_metadata(metadata)\n        try:\n            result = self._col_metadata.insert(aligned_metadata)\n            meta_id = str(result.primary_keys[0])\n        except MilvusException as e:\n            logger.error(f\"Failed to insert meta data\")\n            raise e\n        aligned_docs = align_embeddings(documents, meta_id)\n        try:\n            # As we passed collection_name, documents will be added to existed collection\n            self._col_embeddings = Milvus.from_documents(\n                embedding=self._embedding_func,\n                collection_name=self._embedding_name,\n                connection_args=self._connection_args,\n                documents=aligned_docs,\n            )\n        except MilvusException as e:\n            logger.error(\n                \"Failed to insert data to embedding collection \"\n                f\"{self._embedding_name}.\"\n            )\n            raise e\n        return meta_id\n\n    def store_embeddings(self, documents: List[Document]) -&gt; str:\n        \"\"\"\n        Store documents in the currently active database.\n\n        Args:\n            documents (List[Documents]): documents array, usually from\n                DocumentReader.load_document, DocumentReader.document_from_pdf,\n                DocumentReader.document_from_txt\n\n        Returns:\n            str: document id\n        \"\"\"\n        if len(documents) == 0:\n            return\n        return self._insert_data(documents)\n\n    def _build_embedding_search_expression(\n        self, meta_ids: List[Dict]\n    ) -&gt; Optional[str]:\n        \"\"\"\n        Build search expression for embedding collection. The generated\n        expression follows the pattern: \"meta_id in [{id1}, {id2}, ...]\n\n        Args:\n            meta_ids: the array of metadata id in metadata collection\n\n        Returns:\n            str: search expression or None\n        \"\"\"\n        if len(meta_ids) == 0:\n            return None\n        built_expr = \"\"\"meta_id in [\"\"\"\n        for item in meta_ids:\n            id = f'\"{item[\"id\"]}\",'\n            built_expr += id\n        built_expr = built_expr[:-1]\n        built_expr += \"\"\"]\"\"\"\n        return built_expr\n\n    def _join_embedding_and_metadata_results(\n        self, result_embedding: List[Document], result_meta: List[Dict]\n    ) -&gt; List[Document]:\n        \"\"\"\n        Join the search results of embedding collection and results of metadata.\n\n        Args:\n            result_embedding (List[Document]): search result of embedding\n                collection\n\n            result_meta (List[Dict]): search result of metadata collection\n\n        Returns:\n            List[Document]: combined results like\n                [{page_content: str, metadata: {...}}]\n        \"\"\"\n\n        def _find_metadata_by_id(\n            metadata: List[Dict], id: str\n        ) -&gt; Optional[Dict]:\n            for d in metadata:\n                if str(d[\"id\"]) == id:\n                    return d\n            return None\n\n        joined_docs = []\n        for res in result_embedding:\n            found = _find_metadata_by_id(result_meta, res.metadata[\"meta_id\"])\n            if found is None:  # discard\n                logger.error(\n                    f\"Failed to join meta_id {res.metadata['meta_id']}\"\n                )\n                continue\n            joined_docs.append(\n                Document(page_content=res.page_content, metadata=found)\n            )\n        return joined_docs\n\n    def similarity_search(self, query: str, k: int = 3) -&gt; List[Document]:\n        \"\"\"\n        Perform similarity search insider the currently active database\n        according to the input query.\n\n        This method will:\n        1. get all non-deleted meta_id and build to search expression for\n            the currently active embedding collection\n        2. do similarity search in the embedding collection\n        3. combine metadata and embeddings\n\n        Args:\n            query (str): query string\n\n            k (int): the number of results to return\n\n        Returns:\n            List[Document]: search results\n        \"\"\"\n        result_metadata = self._col_metadata.query(expr=\"isDeleted == false\")\n        expr = self._build_embedding_search_expression(result_metadata)\n        result_embedding = self._col_embeddings.similarity_search(\n            query=query, k=k, expr=expr\n        )\n        return self._join_embedding_and_metadata_results(\n            result_embedding, result_metadata\n        )\n\n    def remove_document(self, doc_id: str) -&gt; bool:\n        \"\"\"\n        Remove the document include meta data and its embeddings.\n\n        Args:\n            doc_id (str): the document to be deleted\n\n        Returns:\n            bool: True if the document is deleted, False otherwise\n        \"\"\"\n        if not self._col_metadata:\n            return False\n        try:\n            expr = f\"id in [{doc_id}]\"\n            res = self._col_metadata.query(\n                expr=expr, output_fields=METADATA_FIELDS\n            )\n            if len(res) == 0:\n                return False\n            del_res = self._col_metadata.delete(expr)\n            self._col_metadata.flush()\n\n            res = self._col_embeddings.col.query(f'meta_id in [\"{doc_id}\"]')\n            if len(res) == 0:\n                return True\n            ids = [item[\"pk\"] for item in res]\n            embedding_expr = f\"pk in {ids}\"\n            del_res = self._col_embeddings.col.delete(expr=embedding_expr)\n            self._col_embeddings.col.flush()\n            return True\n        except MilvusException as e:\n            logger.error(e)\n            raise e\n\n    def get_all_documents(self) -&gt; List[Dict]:\n        \"\"\"\n        Get all non-deleted documents from the currently active database.\n\n        Returns:\n            List[Dict]: the metadata of all non-deleted documents in the form\n                [{{id}, {author}, {source}, ...}]\n        \"\"\"\n        try:\n            result_metadata = self._col_metadata.query(\n                expr=\"isDeleted == false\", output_fields=METADATA_FIELDS\n            )\n            return result_metadata\n        except MilvusException as e:\n            logger.error(e)\n            raise e\n</code></pre>"},{"location":"vectorstore_host-reference/#biochatter.vectorstore_host.VectorDatabaseHostMilvus.connect","title":"<code>connect()</code>","text":"<p>Connect to a host and read two document collections (the default names are <code>DocumentEmbeddings</code> and <code>DocumentMetadata</code>) in the currently active database (default database name is <code>default</code>); if those document collections don't exist, create the two collections.</p> Source code in <code>biochatter/vectorstore_host.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"\n    Connect to a host and read two document collections (the default names\n    are `DocumentEmbeddings` and `DocumentMetadata`) in the currently active\n    database (default database name is `default`); if those document\n    collections don't exist, create the two collections.\n    \"\"\"\n    self._connect(\n        self._connection_args[\"host\"], self._connection_args[\"port\"]\n    )\n    self._init_host()\n</code></pre>"},{"location":"vectorstore_host-reference/#biochatter.vectorstore_host.VectorDatabaseHostMilvus.get_all_documents","title":"<code>get_all_documents()</code>","text":"<p>Get all non-deleted documents from the currently active database.</p> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: the metadata of all non-deleted documents in the form [{{id}, {author}, {source}, ...}]</p> Source code in <code>biochatter/vectorstore_host.py</code> <pre><code>def get_all_documents(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all non-deleted documents from the currently active database.\n\n    Returns:\n        List[Dict]: the metadata of all non-deleted documents in the form\n            [{{id}, {author}, {source}, ...}]\n    \"\"\"\n    try:\n        result_metadata = self._col_metadata.query(\n            expr=\"isDeleted == false\", output_fields=METADATA_FIELDS\n        )\n        return result_metadata\n    except MilvusException as e:\n        logger.error(e)\n        raise e\n</code></pre>"},{"location":"vectorstore_host-reference/#biochatter.vectorstore_host.VectorDatabaseHostMilvus.remove_document","title":"<code>remove_document(doc_id)</code>","text":"<p>Remove the document include meta data and its embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>doc_id</code> <code>str</code> <p>the document to be deleted</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the document is deleted, False otherwise</p> Source code in <code>biochatter/vectorstore_host.py</code> <pre><code>def remove_document(self, doc_id: str) -&gt; bool:\n    \"\"\"\n    Remove the document include meta data and its embeddings.\n\n    Args:\n        doc_id (str): the document to be deleted\n\n    Returns:\n        bool: True if the document is deleted, False otherwise\n    \"\"\"\n    if not self._col_metadata:\n        return False\n    try:\n        expr = f\"id in [{doc_id}]\"\n        res = self._col_metadata.query(\n            expr=expr, output_fields=METADATA_FIELDS\n        )\n        if len(res) == 0:\n            return False\n        del_res = self._col_metadata.delete(expr)\n        self._col_metadata.flush()\n\n        res = self._col_embeddings.col.query(f'meta_id in [\"{doc_id}\"]')\n        if len(res) == 0:\n            return True\n        ids = [item[\"pk\"] for item in res]\n        embedding_expr = f\"pk in {ids}\"\n        del_res = self._col_embeddings.col.delete(expr=embedding_expr)\n        self._col_embeddings.col.flush()\n        return True\n    except MilvusException as e:\n        logger.error(e)\n        raise e\n</code></pre>"},{"location":"vectorstore_host-reference/#biochatter.vectorstore_host.VectorDatabaseHostMilvus.similarity_search","title":"<code>similarity_search(query, k=3)</code>","text":"<p>Perform similarity search insider the currently active database according to the input query.</p> <p>This method will: 1. get all non-deleted meta_id and build to search expression for     the currently active embedding collection 2. do similarity search in the embedding collection 3. combine metadata and embeddings</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query string</p> required <code>k</code> <code>int</code> <p>the number of results to return</p> <code>3</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: search results</p> Source code in <code>biochatter/vectorstore_host.py</code> <pre><code>def similarity_search(self, query: str, k: int = 3) -&gt; List[Document]:\n    \"\"\"\n    Perform similarity search insider the currently active database\n    according to the input query.\n\n    This method will:\n    1. get all non-deleted meta_id and build to search expression for\n        the currently active embedding collection\n    2. do similarity search in the embedding collection\n    3. combine metadata and embeddings\n\n    Args:\n        query (str): query string\n\n        k (int): the number of results to return\n\n    Returns:\n        List[Document]: search results\n    \"\"\"\n    result_metadata = self._col_metadata.query(expr=\"isDeleted == false\")\n    expr = self._build_embedding_search_expression(result_metadata)\n    result_embedding = self._col_embeddings.similarity_search(\n        query=query, k=k, expr=expr\n    )\n    return self._join_embedding_and_metadata_results(\n        result_embedding, result_metadata\n    )\n</code></pre>"},{"location":"vectorstore_host-reference/#biochatter.vectorstore_host.VectorDatabaseHostMilvus.store_embeddings","title":"<code>store_embeddings(documents)</code>","text":"<p>Store documents in the currently active database.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Documents]</code> <p>documents array, usually from DocumentReader.load_document, DocumentReader.document_from_pdf, DocumentReader.document_from_txt</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>document id</p> Source code in <code>biochatter/vectorstore_host.py</code> <pre><code>def store_embeddings(self, documents: List[Document]) -&gt; str:\n    \"\"\"\n    Store documents in the currently active database.\n\n    Args:\n        documents (List[Documents]): documents array, usually from\n            DocumentReader.load_document, DocumentReader.document_from_pdf,\n            DocumentReader.document_from_txt\n\n    Returns:\n        str: document id\n    \"\"\"\n    if len(documents) == 0:\n        return\n    return self._insert_data(documents)\n</code></pre>"},{"location":"vectorstore_host-reference/#biochatter.vectorstore_host.align_embeddings","title":"<code>align_embeddings(docs, meta_id)</code>","text":"<p>Ensure that the metadata id is present in each document.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of documents</p> required <code>meta_id</code> <code>int</code> <p>Metadata id to assign to the documents</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of documents, with each document having a metadata id.</p> Source code in <code>biochatter/vectorstore_host.py</code> <pre><code>def align_embeddings(docs: List[Document], meta_id: int) -&gt; List[Document]:\n    \"\"\"\n    Ensure that the metadata id is present in each document.\n\n    Args:\n        docs (List[Document]): List of documents\n\n        meta_id (int): Metadata id to assign to the documents\n\n    Returns:\n        List[Document]: List of documents, with each document having a metadata\n            id.\n    \"\"\"\n    ret = []\n    for doc in docs:\n        ret.append(\n            Document(\n                page_content=doc.page_content,\n                metadata={\"meta_id\": meta_id},\n            )\n        )\n    return ret\n</code></pre>"},{"location":"vectorstore_host-reference/#biochatter.vectorstore_host.align_metadata","title":"<code>align_metadata(metadata, isDeleted=False)</code>","text":"<p>Ensure that specific metadata fields are present; if not provided, fill with \"unknown\". Also, add a random vector to each metadata item to simulate an embedding.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>List[Dict]</code> <p>List of metadata items</p> required <code>isDeleted</code> <code>Optional[bool]</code> <p>Whether the document is deleted. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[List]</code> <p>List[List]: List of metadata items, with each item being a list of metadata fields.</p> Source code in <code>biochatter/vectorstore_host.py</code> <pre><code>def align_metadata(\n    metadata: List[Dict], isDeleted: Optional[bool] = False\n) -&gt; List[List]:\n    \"\"\"\n\n    Ensure that specific metadata fields are present; if not provided, fill with\n    \"unknown\". Also, add a random vector to each metadata item to simulate an\n    embedding.\n\n    Args:\n        metadata (List[Dict]): List of metadata items\n\n        isDeleted (Optional[bool], optional): Whether the document is deleted.\n            Defaults to False.\n\n    Returns:\n        List[List]: List of metadata items, with each item being a list of\n            metadata fields.\n    \"\"\"\n    ret = []\n    fields = METADATA_FIELDS.copy()\n    fields.pop(0)\n    for ix, k in enumerate(fields):\n        ret.append([item[k] if k in item else \"unknown\" for item in metadata])\n\n    ret.append(\n        [\n            [random.random() for _ in range(METADATA_VECTOR_DIM)]\n            for _ in range(len(metadata))\n        ]\n    )\n    ret.append([isDeleted for _ in metadata])\n    return ret\n</code></pre>"},{"location":"wasm/","title":"LLM in your Browser - WebAssembly","text":"<p>Coming soon.</p>"}]}